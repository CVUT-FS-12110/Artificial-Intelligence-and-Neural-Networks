{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, balanced_accuracy_score,\n",
        "    matthews_corrcoef, roc_auc_score, RocCurveDisplay,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "\n",
        "# Pro reprodukovatelnost\n",
        "np.random.seed(42)\n",
        "RSTATE = 42"
      ],
      "metadata": {
        "id": "Q1I1C9GfZOBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning — Classification\n",
        "\n",
        "## What is Classification?\n",
        "\n",
        "Classification is the second fundamental task in **supervised learning**. Its goal is to **predict a discrete category (class)**.\n",
        "\n",
        "Unlike *regression*, which predicts a continuous value (e.g., 25.5 °C), classification answers questions such as “Yes/No,” “Which type?”, or “Does it belong here?”\n",
        "\n",
        "* **Binary classification:** Predicts one of two classes (e.g., `0` vs `1`, `Spam` vs `Not Spam`, `Malignant tumor` vs `Benign`).\n",
        "* **Multi-class classification:** Predicts one of many classes (e.g., `Dog` vs `Cat` vs `Bird`, `Handwritten digits 0–9`).\n",
        "\n",
        "In this exercise, we will focus on **binary classification** and build one of the oldest and most famous models: **Rosenblatt’s Perceptron**.\n",
        "\n",
        "The goal is to find a **decision boundary** (in 2D, a straight line) that best separates the points of one class from those of the other.\n"
      ],
      "metadata": {
        "id": "LOrC8SeQYi2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical background (Perceptron)\n",
        "\n",
        "The perceptron is a type of linear classifier. It works in two steps:\n",
        "\n",
        "### 1. Compute the “score”\n",
        "Just like in linear regression, the model first computes a weighted sum of the inputs \\(x_i\\) and adds a bias \\(b\\). We call this raw value the score \\(s_i\\).\n",
        "$$\n",
        "s_i = w^\\top x_i + b\n",
        "$$\n",
        "* If \\(s_i\\) is a **large positive number**, the model is “confident” the sample belongs to class +1.\n",
        "* If \\(s_i\\) is a **large negative number**, the model is “confident” the sample belongs to class −1.\n",
        "* If \\(s_i\\) is **close to zero**, the sample lies near the decision boundary.\n",
        "\n",
        "### 2. Activation function (Step Function)\n",
        "To obtain the final class prediction (e.g., `0` or `1`), we pass the score \\(s_i\\) through a **step activation function**.  \n",
        "For predicting classes \\(\\{-1, +1\\}\\) we use the \\(\\text{sign}\\) function:\n",
        "$$\n",
        "\\hat{y}_i = \\text{sign}(s_i) = \\begin{cases}\n",
        "      +1 & \\text{if } s_i \\ge 0 \\\\\n",
        "      -1 & \\text{if } s_i < 0\n",
        "   \\end{cases}\n",
        "$$\n",
        "(For classes \\(\\{0,1\\}\\), we would use \\(\\hat{y}_i = 1\\) if \\(s_i \\ge 0\\), otherwise \\(0\\).)\n",
        "\n",
        "### 3. Training (Perceptron Learning Rule)\n",
        "The perceptron learns using a procedure similar to **Stochastic Gradient Descent (SGD)**. It processes the data sample by sample.\n",
        "\n",
        "* If the prediction \\(\\hat{y}_i\\) is **correct** (i.e., \\(\\hat{y}_i = y_i\\)), **nothing happens**.\n",
        "* If the prediction \\(\\hat{y}_i\\) is **incorrect** (i.e., \\(\\hat{y}_i \\ne y_i\\)), the model **immediately updates the weights**:\n",
        "\n",
        "For learning rate \\(\\eta\\) and target \\(y_i \\in \\{-1, +1\\}\\):\n",
        "$$\n",
        "w \\leftarrow w + \\eta \\cdot (y_i - \\hat{y}_i) \\cdot x_i\n",
        "$$\n",
        "$$\n",
        "b \\leftarrow b + \\eta \\cdot (y_i - \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "(Note: Because \\((y_i - \\hat{y}_i)\\) is always \\(\\pm 2\\), this is often simplified to  \n",
        "\\( w \\leftarrow w + \\eta \\cdot y_i \\cdot x_i \\) for \\(y_i \\in \\{-1,+1\\}\\) if \\(\\eta\\) absorbs the constant. We will keep the more general form, which also works for 0/1 labels.)\n",
        "\n",
        "**Important property:** The perceptron is guaranteed to find a solution (converge) **if and only if** the data are **linearly separable** (i.e., there exists a straight line that separates the classes). If the data are not linearly separable, the algorithm will keep “jumping” around forever without convergence.\n",
        "\n",
        "$$\\phantom{}$$\n"
      ],
      "metadata": {
        "id": "mfqjblUGYzR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptronClassifier:\n",
        "    \"\"\"\n",
        "    Implementation of Rosenblatt's perceptron.\n",
        "\n",
        "    Trained using an \"online\" (stochastic) method, i.e., sample by sample.\n",
        "    Accepts target labels as {0, 1}, but internally works with {-1, +1}.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_epochs: int = 100, lr: float = 0.01,\n",
        "                 fit_intercept: bool = True,\n",
        "                 early_stopping_patience: int = 15):\n",
        "        \"\"\"\n",
        "        Initialize hyperparameters.\n",
        "\n",
        "        n_epochs: Maximum number of epochs (full passes over the data)\n",
        "        lr: Learning rate\n",
        "        fit_intercept: Whether to learn 'b' (bias/intercept)\n",
        "        early_stopping_patience: Number of epochs without improvement on the\n",
        "                                 validation set before training stops.\n",
        "        \"\"\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "\n",
        "        self.w = None\n",
        "        self.b = 0.0\n",
        "        # We can also store the error history for debugging/tuning\n",
        "        self.errors_history = []\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray,\n",
        "            X_val: np.ndarray = None, y_val: np.ndarray = None):\n",
        "        \"\"\"\n",
        "        Train the model on (X, y), optionally using (X_val, y_val)\n",
        "        for early stopping.\n",
        "\n",
        "        y: Expects labels {0, 1}\n",
        "        \"\"\"\n",
        "        n, d = X.shape\n",
        "        self.w = np.zeros(d, dtype=float)\n",
        "        self.b = 0.0\n",
        "\n",
        "        # Convert labels from {0, 1} to {-1, +1} for training\n",
        "        y_pm = np.where(y == 1, 1, -1)\n",
        "\n",
        "        # --- Early Stopping setup ---\n",
        "        use_es = X_val is not None and y_val is not None\n",
        "        best_val_acc = -np.inf\n",
        "        patience_counter = 0\n",
        "        best_w = self.w.copy()\n",
        "        best_b = self.b\n",
        "        # ----------------------------\n",
        "\n",
        "        print(f\"Perceptron training starts (max {self.n_epochs} epochs, lr={self.lr})...\")\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            errors_in_epoch = 0\n",
        "\n",
        "            # Training proceeds sample-by-sample (Stochastic/Online)\n",
        "            for xi, target in zip(X, y_pm):\n",
        "                # 1. Compute score\n",
        "                s = float(np.dot(self.w, xi) + (self.b if self.fit_intercept else 0.0))\n",
        "\n",
        "                # 2. Prediction via step function\n",
        "                y_hat = 1 if s >= 0.0 else -1\n",
        "\n",
        "                # 3. Update weights ONLY when misclassified\n",
        "                if y_hat != target:\n",
        "                    errors_in_epoch += 1\n",
        "                    # Update rule: w = w + lr * y_i * x_i\n",
        "                    # (y_i is 'target', which is -1 or +1)\n",
        "                    upd = self.lr * target\n",
        "                    self.w += upd * xi\n",
        "                    if self.fit_intercept:\n",
        "                        self.b += upd\n",
        "\n",
        "            self.errors_history.append(errors_in_epoch)\n",
        "\n",
        "            # --- Early Stopping check (end of epoch) ---\n",
        "            if use_es:\n",
        "                # For ES we must use .predict(), which returns {0, 1}\n",
        "                val_acc = accuracy_score(y_val, self.predict(X_val))\n",
        "\n",
        "                if val_acc > best_val_acc + 1e-4:  # Tolerance for improvement\n",
        "                    best_val_acc = val_acc\n",
        "                    best_w = self.w.copy()\n",
        "                    best_b = self.b\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= self.early_stopping_patience:\n",
        "                    print(f\"INFO: Early stopping at epoch {epoch + 1} (Val Acc: {best_val_acc:.4f})\")\n",
        "                    break\n",
        "            # -------------------------------------------\n",
        "\n",
        "        # --- End of training ---\n",
        "        if use_es:\n",
        "            print(f\"INFO: Training finished. Restoring model from epoch {epoch + 1 - patience_counter}.\")\n",
        "            self.w = best_w\n",
        "            self.b = best_b\n",
        "        else:\n",
        "            print(\"INFO: Training finished (reached max epochs).\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Return the raw score (before the activation function).\n",
        "        This is key for computing ROC–AUC.\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise RuntimeError(\"Model is not trained. Call 'fit' first.\")\n",
        "\n",
        "        return X @ self.w + (self.b if self.fit_intercept else 0.0)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform final classification on new data X.\n",
        "        Returns labels {0, 1}.\n",
        "        \"\"\"\n",
        "        # Compute scores\n",
        "        scores = self.decision_function(X)\n",
        "\n",
        "        # Apply the step function and convert to int (0 or 1)\n",
        "        return (scores >= 0.0).astype(int)\n"
      ],
      "metadata": {
        "id": "QV5bj2gxYyLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation (Classification)\n",
        "\n",
        "For classification, we will generate 2D data using `make_blobs`.  \n",
        "This will create two “clusters” of points that represent our two classes (`0` and `1`).\n",
        "\n",
        "### Key Concept: Stratification\n",
        "\n",
        "When splitting data into `train`, `validation`, and `test` sets, it is **extremely important** in classification tasks to use **stratification**.\n",
        "\n",
        "**The Problem:**  \n",
        "Imagine you have a dataset with 100 samples, where 90 belong to class `A` and only 10 to class `B` (an *imbalanced dataset*).  \n",
        "If you split the data randomly, it could happen that *all* samples of class `B` end up in the training set — leaving **zero samples of class `B`** in the test set.  \n",
        "In that case, your test performance metric would be meaningless.\n",
        "\n",
        "**The Solution (Stratification):**  \n",
        "The argument `stratify=y` in `train_test_split` ensures that the **class proportions remain the same** in the training, validation, and test sets.\n",
        "\n",
        "As always, we also **standardize** the data so that both the Perceptron model and `StandardScaler` work under consistent, well-scaled inputs.\n"
      ],
      "metadata": {
        "id": "Je1MTAqGZqC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data generation\n",
        "# Create 500 points split into 2 centers (classes)\n",
        "# cluster_std=1.6: How dispersed the points are\n",
        "X_cls, y_cls = make_blobs(n_samples=500, centers=2, cluster_std=1.6,\n",
        "                          random_state=RSTATE, center_box=(-5, 5))\n",
        "\n",
        "# 2. Split into Train (60%), Validation (20%), Test (20%)\n",
        "# We use stratify=y_cls and stratify=yc_tmp to preserve class proportions\n",
        "\n",
        "# Step 1: Train (60%) and Temporary (40%)\n",
        "Xc_tr, Xc_tmp, yc_tr, yc_tmp = train_test_split(\n",
        "    X_cls, y_cls, test_size=0.4, random_state=RSTATE, stratify=y_cls\n",
        ")\n",
        "\n",
        "# Step 2: Validation (20%) and Test (20%)\n",
        "Xc_va, Xc_te, yc_va, yc_te = train_test_split(\n",
        "    Xc_tmp, yc_tmp, test_size=0.5, random_state=RSTATE, stratify=yc_tmp\n",
        ")\n",
        "\n",
        "print(\"--- Dataset shapes (Classification) ---\")\n",
        "print(f\"Train:      {Xc_tr.shape}, {yc_tr.shape}\")\n",
        "print(f\"Validation: {Xc_va.shape}, {yc_va.shape}\")\n",
        "print(f\"Test:       {Xc_te.shape}, {yc_te.shape}\")\n",
        "\n",
        "# Verify stratification (percentage of class 1)\n",
        "print(\"\\n--- Stratification check (% of class 1) ---\")\n",
        "print(f\"Overall:    {np.mean(y_cls):.2f}\")\n",
        "print(f\"Train:      {np.mean(yc_tr):.2f}\")\n",
        "print(f\"Validation: {np.mean(yc_va):.2f}\")\n",
        "print(f\"Test:       {np.mean(yc_te):.2f}\")\n",
        "\n",
        "# 3. Standardization (Scaling) of data\n",
        "# Fit the scaler ONLY on the training data (Xc_tr)\n",
        "sc_c = StandardScaler().fit(Xc_tr)\n",
        "\n",
        "# Apply the trained scaler to all splits\n",
        "Xc_tr_s = sc_c.transform(Xc_tr)\n",
        "Xc_va_s = sc_c.transform(Xc_va)\n",
        "Xc_te_s = sc_c.transform(Xc_te)\n"
      ],
      "metadata": {
        "id": "a77BfQH9ZyFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visualization\n",
        "\n",
        "# Plot the training set (before scaling)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot each class separately\n",
        "plt.scatter(Xc_tr[yc_tr == 0][:, 0], Xc_tr[yc_tr == 0][:, 1],\n",
        "            c='blue', label='Class 0', alpha=0.7)\n",
        "plt.scatter(Xc_tr[yc_tr == 1][:, 0], Xc_tr[yc_tr == 1][:, 1],\n",
        "            c='orange', label='Class 1', alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.title(\"Generated classification data (Train set)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.axhline(0, color='black', lw=0.5)\n",
        "plt.axvline(0, color='black', lw=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OXTjQBQDZ8jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating the model\n",
        "\n",
        "# 1. Initialize the model\n",
        "# n_epochs=500: Set a high maximum; Early Stopping will likely stop earlier.\n",
        "# lr=0.01: Relatively small learning rate for more stable convergence.\n",
        "# early_stopping_patience=20: Stop training if Validation Accuracy\n",
        "#                              does not improve for 20 consecutive epochs.\n",
        "clf = PerceptronClassifier(\n",
        "    n_epochs=500,\n",
        "    lr=0.01,\n",
        "    early_stopping_patience=20\n",
        ")\n",
        "\n",
        "# 2. Train the model\n",
        "# Train on scaled training data (Xc_tr_s, yc_tr)\n",
        "# and use scaled validation data (Xc_va_s, yc_va) for Early Stopping.\n",
        "clf.fit(Xc_tr_s, yc_tr, X_val=Xc_va_s, y_val=yc_va)\n",
        "\n",
        "# 3. Predictions on the TEST set\n",
        "# Final class predictions (returns 0 or 1)\n",
        "yc_pred_te = clf.predict(Xc_te_s)\n",
        "# Raw scores (needed for ROC–AUC)\n",
        "scores_te = clf.decision_function(Xc_te_s)\n",
        "\n",
        "\n",
        "# 4. Evaluation on the TEST set\n",
        "print(\"\\n\" + \"---\" * 15)\n",
        "print(\"Final performance on the TEST set:\")\n",
        "print(\"---\" * 15)\n",
        "\n",
        "# Basic metrics\n",
        "acc  = accuracy_score(yc_te, yc_pred_te)\n",
        "prec = precision_score(yc_te, yc_pred_te, zero_division=0)\n",
        "rec  = recall_score(yc_te, yc_pred_te, zero_division=0)\n",
        "f1   = f1_score(yc_te, yc_pred_te, zero_division=0)\n",
        "\n",
        "# Advanced metrics\n",
        "bal_acc = balanced_accuracy_score(yc_te, yc_pred_te)\n",
        "mcc = matthews_corrcoef(yc_te, yc_pred_te)\n",
        "# ROC–AUC must be computed from raw scores, not hard 0/1 predictions\n",
        "roc_auc = roc_auc_score(yc_te, scores_te)\n",
        "\n",
        "print(f\"Accuracy:          {acc:.3f}\")\n",
        "print(f\"Precision:         {prec:.3f} (Positive Predictive Value for class 1)\")\n",
        "print(f\"Recall:            {rec:.3f} (Sensitivity/True Positive Rate for class 1)\")\n",
        "print(f\"F1 Score:          {f1:.3f} (Harmonic mean of Precision and Recall)\")\n",
        "print(\"---\")\n",
        "print(f\"Balanced Accuracy: {bal_acc:.3f} (Better for imbalanced data)\")\n",
        "print(f\"Matthews CC:       {mcc:.3f} (Robust metric, range -1 to +1)\")\n",
        "print(f\"ROC–AUC:           {roc_auc:.3f} (Area under the ROC curve)\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(yc_te, yc_pred_te)\n",
        "print(cm)\n",
        "print(f\"(True Negatives predicted as Negative: {cm[0, 0]})\")\n",
        "print(f\"(True Positives predicted as Positive: {cm[1, 1]})\")\n",
        "\n",
        "# Classification Report (compact summary)\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "report = classification_report(yc_te, yc_pred_te, digits=3, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "vPR3h1xfaEmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Classification Metrics\n",
        "\n",
        "With regression, evaluation was straightforward (RMSE, R²).  \n",
        "For classification, evaluation is more complex and **the choice of metric depends on the problem you are solving**.\n",
        "\n",
        "All classification metrics (except ROC–AUC) are derived from the **Confusion Matrix**.\n",
        "\n",
        "### Confusion Matrix Terms\n",
        "\n",
        "* **True Positive (TP):** Actual = `1` (Positive), model predicted `1` (hit).\n",
        "* **True Negative (TN):** Actual = `0` (Negative), model predicted `0` (hit).\n",
        "* **False Positive (FP) / Type I Error:** Actual = `0`, model predicted `1`  \n",
        "  (false alarm — e.g., a legitimate email marked as spam).\n",
        "* **False Negative (FN) / Type II Error:** Actual = `1`, model predicted `0`  \n",
        "  (miss — e.g., a spam email delivered to inbox, or cancer labeled as benign).\n",
        "\n",
        "---\n",
        "\n",
        "### Basic Metrics\n",
        "\n",
        "* **Accuracy:** $\\frac{TP + TN}{TP + TN + FP + FN}$  \n",
        "  * **Meaning:** “What percentage of all predictions were correct?”  \n",
        "  * **Issue:** **Misleading on imbalanced data.**  \n",
        "    Example: if 99% of samples are class `A`, a model always predicting `A` will have 99% accuracy and still be useless.\n",
        "\n",
        "* **Precision (for class 1):** $\\frac{TP}{TP + FP}$  \n",
        "  * **Meaning:** “Of all samples predicted as Positive (`1`), how many truly were Positive?”  \n",
        "  * **When important:** Avoiding **False Positives (FP)**.  \n",
        "  * **Example:** Spam filter — high precision means very few important emails are wrongly flagged.\n",
        "\n",
        "* **Recall (Sensitivity):** $\\frac{TP}{TP + FN}$  \n",
        "  * **Meaning:** “Of all true Positive samples, how many did the model detect?”  \n",
        "  * **When important:** Avoiding **False Negatives (FN)**.  \n",
        "  * **Example:** Cancer screening — better to flag extra healthy people than miss one sick patient.\n",
        "\n",
        "* **F1-Score:**  \n",
        "  $$\n",
        "  2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  $$  \n",
        "  * **Meaning:** Harmonic mean of Precision and Recall. Good when you need **balance** between them.\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Metrics\n",
        "\n",
        "* **Balanced Accuracy:**  \n",
        "  $$\n",
        "  \\frac{\\text{Recall for class 0} + \\text{Recall for class 1}}{2}\n",
        "  $$  \n",
        "  * **Meaning:** Average detection rate across both classes. Much better than Accuracy for **imbalanced data**.\n",
        "\n",
        "* **Matthews Correlation Coefficient (MCC):**  \n",
        "  * **Meaning:** A correlation-like score using TP, TN, FP, FN.  \n",
        "  * **Interpretation:**  \n",
        "    * `+1`: perfect classifier  \n",
        "    * `0`: random guessing  \n",
        "    * `-1`: perfectly wrong (inverse predictions)  \n",
        "  * **Strength:** One of the most robust metrics for imbalanced classification.\n",
        "\n",
        "* **ROC–AUC:** (see next section)  \n",
        "  * **Meaning:** “Probability that the model assigns a higher **score** to a random Positive than to a random Negative.”  \n",
        "  * **Note:** Evaluates **ranking ability**, not just hard 0/1 predictions.  \n",
        "  * `1.0` = perfect, `0.5` = random.\n",
        "\n",
        "$$\\phantom{}$$\n"
      ],
      "metadata": {
        "id": "a-tlLIquZpOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Decision Boundary\n",
        "\n",
        "# Setup for plotting the boundary\n",
        "# Create a grid of points covering the whole plot area\n",
        "x0_min, x0_max = Xc_te_s[:, 0].min() - 0.5, Xc_te_s[:, 0].max() + 0.5\n",
        "x1_min, x1_max = Xc_te_s[:, 1].min() - 0.5, Xc_te_s[:, 1].max() + 0.5\n",
        "\n",
        "# np.meshgrid creates coordinates for every point in the grid\n",
        "xx0, xx1 = np.meshgrid(\n",
        "    np.linspace(x0_min, x0_max, 200),\n",
        "    np.linspace(x1_min, x1_max, 200)\n",
        ")\n",
        "\n",
        "# Create one large 2D array from all grid points\n",
        "grid_points = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "\n",
        "# Compute prediction (0 or 1) for every grid point\n",
        "zz = clf.predict(grid_points).reshape(xx0.shape)\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# 1. Plot colored “zones” of predictions\n",
        "# contourf fills the area with colors according to values in 'zz'\n",
        "plt.contourf(xx0, xx1, zz, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "\n",
        "# 2. Plot test points\n",
        "# (c=yc_te) colors points by their TRUE class\n",
        "plt.scatter(Xc_te_s[:, 0], Xc_te_s[:, 1], c=yc_te,\n",
        "            cmap=plt.cm.coolwarm, edgecolors='k', label=\"Test data\")\n",
        "\n",
        "plt.xlabel(\"Feature 1 (standardized)\")\n",
        "plt.ylabel(\"Feature 2 (standardized)\")\n",
        "plt.title(\"Perceptron — Decision Boundary (Test set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LAc1bgsJaqRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized Confusion Matrix\n",
        "\n",
        "# 'cm' is already computed in the previous cell:\n",
        "# cm = confusion_matrix(yc_te, yc_pred_te)\n",
        "\n",
        "# Row-wise normalization (relative to the true class)\n",
        "# cm.sum(axis=1, keepdims=True) computes the sum of each row\n",
        "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Display matrix as heatmap\n",
        "im = ax.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "\n",
        "# Add color bar\n",
        "plt.colorbar(im)\n",
        "\n",
        "# Axis settings\n",
        "ax.set(\n",
        "    xticks=np.arange(cm.shape[1]),\n",
        "    yticks=np.arange(cm.shape[0]),\n",
        "    xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "    yticklabels=['Actual 0', 'Actual 1'],\n",
        "    title=\"Confusion Matrix (Normalized by True Class)\",\n",
        "    ylabel='True Class',\n",
        "    xlabel='Predicted Class'\n",
        ")\n",
        "\n",
        "# Insert numeric values (percentages) in each cell\n",
        "thresh = cm_norm.max() / 2.0\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(\n",
        "            j, i, f\"{cm_norm[i, j]:.2f}\",\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\" if cm_norm[i, j] > thresh else \"black\"\n",
        "        )\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ab1a-IPSa-7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC Curve and ROC-AUC\n",
        "\n",
        "This is one of the most important evaluation tools for binary classification.\n",
        "\n",
        "### What Is a Threshold?\n",
        "\n",
        "Our model does not directly output `0` or `1`. It first produces a **score** (via the `decision_function`). By default:\n",
        "\n",
        "* If `score >= 0`, predict class `1`\n",
        "* If `score < 0`, predict class `0`\n",
        "\n",
        "What happens if we shift this threshold?\n",
        "\n",
        "* **High threshold (e.g., +2.0):**  \n",
        "  The model is very \"conservative\" when predicting `1`. It will make few False Positives (FP), but it will also miss many true positives (high FN).  \n",
        "  **Result:** High Precision, low Recall.\n",
        "\n",
        "* **Low threshold (e.g., –2.0):**  \n",
        "  The model will predict `1` for almost everything. It will find nearly all true positives (high Recall), but generate many false alarms (high FP).  \n",
        "  **Result:** Low Precision, high Recall.\n",
        "\n",
        "### ROC Curve\n",
        "\n",
        "The **ROC (Receiver Operating Characteristic)** curve visualizes this *trade-off* across **all possible thresholds**.\n",
        "\n",
        "* **Y-axis:** **True Positive Rate (TPR)** — same as **Recall**  \n",
        "  $$TPR = \\frac{TP}{TP + FN}$$  \n",
        "  (\"What percentage of actual positives did we correctly identify?\")\n",
        "\n",
        "* **X-axis:** **False Positive Rate (FPR)**  \n",
        "  $$FPR = \\frac{FP}{FP + TN}$$  \n",
        "  (\"What percentage of negatives did we incorrectly flag as positive?\")\n",
        "\n",
        "### How to Read the ROC Curve\n",
        "\n",
        "* **Perfect model:** Curve hugs the top-left corner (TPR = 1.0, FPR = 0).  \n",
        "  Means: There exists a threshold where the model finds all positives without any false alarms.\n",
        "* **Our model (orange curve):**  \n",
        "  Shows how FPR increases as we push TPR higher.\n",
        "* **Random baseline (dashed diagonal):**  \n",
        "  Represents a coin-flip classifier. The model must perform significantly above this line.\n",
        "\n",
        "### What Is ROC-AUC?\n",
        "\n",
        "**AUC = Area Under the Curve.**\n",
        "\n",
        "It compresses the entire ROC curve into a single number:\n",
        "\n",
        "| AUC Value | Interpretation          |\n",
        "|-----------|-------------------------|\n",
        "| **1.0**   | Perfect classifier      |\n",
        "| **0.9+**  | Excellent               |\n",
        "| **0.8–0.9** | Very good              |\n",
        "| **0.7–0.8** | Decent                 |\n",
        "| **0.5**   | Random guessing         |\n",
        "\n",
        "**Key idea:**  \n",
        "AUC measures how well the model **ranks** samples, not just the final `0/1` decisions.\n",
        "\n",
        "> \"What is the probability that a randomly chosen positive sample receives a higher score than a randomly chosen negative sample?\"\n",
        "\n",
        "$$\\phantom{}$$\n"
      ],
      "metadata": {
        "id": "dPiJOQXheQZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for ROC Curve\n",
        "\n",
        "# We already have:\n",
        "#  - 'scores_te' (raw scores from decision_function)\n",
        "#  - 'yc_te' (true labels)\n",
        "#  - 'roc_auc' (computed AUC)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Use RocCurveDisplay for convenience\n",
        "# 'from_predictions' takes the true labels and predicted scores\n",
        "RocCurveDisplay.from_predictions(\n",
        "    yc_te,\n",
        "    scores_te,\n",
        "    name=f\"Perceptron (AUC = {roc_auc:.3f})\",\n",
        "    ax=plt.gca()  # gca = \"get current axis\"\n",
        ")\n",
        "\n",
        "# Add diagonal reference line (random model)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random (AUC = 0.5)\")\n",
        "\n",
        "plt.title(\"ROC Curve (Test Set)\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR / Recall)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5M8ubQ4LePPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision–Recall Curve (PR Curve)\n",
        "\n",
        "The ROC–AUC curve is great, but it has one “weakness”: it can be overly optimistic on **highly imbalanced datasets**.\n",
        "\n",
        "**Problem:** On the ROC curve, the X-axis is the False Positive Rate \\((FPR = \\frac{FP}{FP + TN})\\).  \n",
        "If you have 1,000,000 negatives (TN) and only 100 positives (TP), even 10,000 false alarms (FP) yield a *very low* FPR \\((10{,}000 / 1{,}000{,}000 = 0.01)\\).  \n",
        "The model can look excellent on ROC, even though it produces 100× more false alarms than the number of true positives.\n",
        "\n",
        "### PR Curve\n",
        "\n",
        "The Precision–Recall (PR) curve addresses this issue. It shows the same threshold **trade-off**, but with:\n",
        "\n",
        "* **Y-axis:** **Precision**  \n",
        "  $$\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  $$  \n",
        "  (What percentage of *predicted positives* were correct?)\n",
        "\n",
        "* **X-axis:** **Recall** (a.k.a. TPR)  \n",
        "  $$\n",
        "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "  $$  \n",
        "  (What percentage of *actual positives* did we find?)\n",
        "\n",
        "### How to read a PR curve\n",
        "\n",
        "* **Ideal model:** The curve hugs the top-right corner (Precision = 1.0, Recall = 1.0).  \n",
        "  This means the model finds all positives without any false alarms.\n",
        "* **Random baseline (dashed):** A horizontal line at the **prevalence** (fraction of positives).  \n",
        "  (e.g., if 10% of the data are positive, a random classifier has Precision ≈ 0.1.)\n",
        "* **Trade-off:** As you push **Recall** higher (by lowering the threshold), you typically **lose Precision** (FP increases).\n",
        "\n",
        "### What is AP?\n",
        "\n",
        "**AP = Average Precision** (also called PR–AUC). It is the area under the PR curve.  \n",
        "Like ROC–AUC, it summarizes the entire curve into a single number. It is preferred for **imbalanced datasets**, where we primarily care about the minority (positive) class.\n",
        "\n",
        "$$\\phantom{}$$\n"
      ],
      "metadata": {
        "id": "9YCSG0wJer29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for PR curve\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Compute PR curve points\n",
        "# precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
        "pr, rc, _ = precision_recall_curve(yc_te, scores_te)\n",
        "\n",
        "# Compute area under the PR curve (Average Precision)\n",
        "ap = average_precision_score(yc_te, scores_te)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Plot PR curve\n",
        "plt.plot(rc, pr, label=f\"Perceptron (AP = {ap:.3f})\")\n",
        "\n",
        "# Add baseline of a random classifier\n",
        "# (level = proportion of the positive class)\n",
        "random_baseline = np.mean(yc_te)\n",
        "plt.axhline(random_baseline, color='k', linestyle='--',\n",
        "            label=f\"Random baseline (AP = {random_baseline:.3f})\")\n",
        "\n",
        "plt.title(\"Precision–Recall Curve (Test set)\")\n",
        "plt.xlabel(\"Recall (TPR / Sensitivity)\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.xlim(0.0, 1.0)\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ghZfmdYCef7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified K-Fold Cross-Validation\n",
        "\n",
        "Just like in regression, a simple `train/val/test` split is subject to randomness.  \n",
        "In classification it’s even worse — what if the validation split happens to contain only “easy” examples?\n",
        "\n",
        "We therefore use **Stratified K-Fold CV**. It works like standard K-Fold CV, with one key improvement: **each fold preserves the same class proportions** as in the full dataset.\n",
        "\n",
        "This is essential for imbalanced datasets.\n",
        "\n",
        "**Key rule (preventing Data Leakage):** The scaler must **always be fit anew inside each fold** using only that fold’s training split.\n"
      ],
      "metadata": {
        "id": "qlsu0mDLfIdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Stratified K-Fold CV\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# For CV, use the original unscaled data\n",
        "# (combine training and validation sets; keep the test set aside)\n",
        "X_cv_data = np.vstack((Xc_tr, Xc_va))\n",
        "y_cv_data = np.concatenate((yc_tr, yc_va))\n",
        "\n",
        "# Initialize the Stratified K-Fold splitter\n",
        "# Important: 'shuffle=True' is good practice\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RSTATE)\n",
        "\n",
        "cv_acc_scores = []\n",
        "cv_f1_scores = []\n",
        "cv_roc_auc_scores = []\n",
        "fold_counter = 1\n",
        "\n",
        "print(\"Starting 5-Fold Stratified Cross-Validation...\")\n",
        "# Loop over 5 folds\n",
        "# The .split() method needs both X and y to stratify by y\n",
        "for tr_idx, va_idx in skf.split(X_cv_data, y_cv_data):\n",
        "    # 1. Split data into the current fold\n",
        "    Xtr_fold, Xva_fold = X_cv_data[tr_idx], X_cv_data[va_idx]\n",
        "    ytr_fold, yva_fold = y_cv_data[tr_idx], y_cv_data[va_idx]\n",
        "\n",
        "    # 2. FIT THE SCALER ONLY ON THE TRAIN SPLIT OF THE FOLD\n",
        "    sc_fold = StandardScaler().fit(Xtr_fold)\n",
        "\n",
        "    # 3. Transform both splits\n",
        "    Xtr_s_fold = sc_fold.transform(Xtr_fold)\n",
        "    Xva_s_fold = sc_fold.transform(Xva_fold)\n",
        "\n",
        "    # 4. Train the model (without Early Stopping, fixed number of epochs)\n",
        "    mdl_fold = PerceptronClassifier(n_epochs=100, lr=0.01)\n",
        "    mdl_fold.fit(Xtr_s_fold, ytr_fold)  # Train without a validation split\n",
        "\n",
        "    # 5. Evaluate on the validation fold\n",
        "    y_pred_fold = mdl_fold.predict(Xva_s_fold)\n",
        "    # For ROC–AUC we need scores\n",
        "    y_scores_fold = mdl_fold.decision_function(Xva_s_fold)\n",
        "\n",
        "    # Store metrics\n",
        "    acc = accuracy_score(yva_fold, y_pred_fold)\n",
        "    f1 = f1_score(yva_fold, y_pred_fold, zero_division=0)\n",
        "    roc_auc = roc_auc_score(yva_fold, y_scores_fold)\n",
        "\n",
        "    print(f\"  Fold {fold_counter}/5 | Acc: {acc:.3f} | F1: {f1:.3f} | ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "    cv_acc_scores.append(acc)\n",
        "    cv_f1_scores.append(f1)\n",
        "    cv_roc_auc_scores.append(roc_auc)\n",
        "    fold_counter += 1\n",
        "\n",
        "# --- Final CV result ---\n",
        "print(\"---\" * 10)\n",
        "print(\"Stratified K-Fold CV result (mean of 5 folds):\")\n",
        "print(f\"  Mean Accuracy: {np.mean(cv_acc_scores):.3f} (± {np.std(cv_acc_scores):.3f})\")\n",
        "print(f\"  Mean F1-Score: {np.mean(cv_f1_scores):.3f} (± {np.std(cv_f1_scores):.3f})\")\n",
        "print(f\"  Mean ROC–AUC:  {np.mean(cv_roc_auc_scores):.3f} (± {np.std(cv_roc_auc_scores):.3f})\")\n"
      ],
      "metadata": {
        "id": "oskiyCLNesU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with Scikit-learn Models\n",
        "\n",
        "Our hand-built `PerceptronClassifier` is great for understanding first principles.  \n",
        "In practice, we use optimized and more robust implementations from `scikit-learn`.\n",
        "\n",
        "For linear classification, two key options are:\n",
        "\n",
        "* **`LogisticRegression`**\n",
        "  * **What it is:** Despite the name, this is a **classification** model. It is the most widely used and fundamental linear classifier.\n",
        "  * **How it works:** Instead of the perceptron’s step function, it uses the **sigmoid (logistic) function**, which returns a continuous probability in \\([0,1]\\).\n",
        "  * **Advantage:** More stable training via a **smooth loss** (log loss / cross-entropy), and the probabilistic output is often more useful than a hard `0/1`.\n",
        "\n",
        "* **`SGDClassifier`**\n",
        "  * **What it is:** A flexible wrapper that trains various linear models (including the perceptron) using **Stochastic Gradient Descent (SGD)**.\n",
        "  * **How to use:**\n",
        "    * `SGDClassifier(loss='perceptron')`: A near-equivalent to our perceptron.\n",
        "    * `SGDClassifier(loss='log_loss')`: A logistic-regression–style classifier trained with SGD (great for large datasets).\n",
        "    * `SGDClassifier(loss='hinge')`: A **linear SVM–style** classifier (hinge loss).\n",
        "  * **Advantage:** Extremely fast and scalable to massive datasets that don’t fit in memory.\n",
        "\n",
        "Let’s compare our model to `LogisticRegression` and to `SGDClassifier(loss='perceptron')`.\n"
      ],
      "metadata": {
        "id": "4SfFYnUWfjp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Code to compare with Scikit-learn\n",
        "\n",
        "# We use a Pipeline to ensure scikit-learn models\n",
        "# are trained on properly scaled data.\n",
        "\n",
        "# --- Model 1: Logistic Regression ---\n",
        "# 'solver=\"lbfgs\"' is a standard, robust choice\n",
        "# 'max_iter=500' to ensure convergence\n",
        "pipe_logreg = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('model', LogisticRegression(solver='lbfgs', max_iter=500, random_state=RSTATE))\n",
        "])\n",
        "pipe_logreg.fit(Xc_tr, yc_tr)  # Train on the original (unscaled) data; scaler is inside the pipeline\n",
        "y_pred_logreg = pipe_logreg.predict(Xc_te)\n",
        "# For ROC–AUC we need class-1 probabilities\n",
        "y_scores_logreg = pipe_logreg.predict_proba(Xc_te)[:, 1]\n",
        "\n",
        "# --- Model 2: SGDClassifier (Perceptron) ---\n",
        "# 'loss=\"perceptron\"': request a perceptron-like update rule\n",
        "# 'early_stopping=True, n_iter_no_change=15': built-in early stopping\n",
        "# 'max_iter=2000': maximum number of epochs\n",
        "pipe_sgd_perc = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('model', SGDClassifier(loss='perceptron', max_iter=2000,\n",
        "                            early_stopping=True, n_iter_no_change=15,\n",
        "                            random_state=RSTATE))\n",
        "])\n",
        "pipe_sgd_perc.fit(Xc_tr, yc_tr)\n",
        "y_pred_sgd = pipe_sgd_perc.predict(Xc_te)\n",
        "# SGD Perceptron exposes a decision_function suitable for ROC–AUC\n",
        "y_scores_sgd = pipe_sgd_perc.decision_function(Xc_te)\n",
        "\n",
        "# --- Metric comparison ---\n",
        "print(\"--- Performance comparison on the Test set ---\")\n",
        "print(\"Metric            | Our model | LogisticReg | SGD Perceptron\")\n",
        "print(\"------------------|-----------|-------------|----------------\")\n",
        "print(f\"Accuracy:         | {acc:.3f}     | {accuracy_score(yc_te, y_pred_logreg):.3f}        | {accuracy_score(yc_te, y_pred_sgd):.3f}\")\n",
        "print(f\"F1-Score:         | {f1:.3f}     | {f1_score(yc_te, y_pred_logreg):.3f}        | {f1_score(yc_te, y_pred_sgd):.3f}\")\n",
        "print(f\"ROC-AUC:          | {roc_auc:.3f}     | {roc_auc_score(yc_te, y_scores_logreg):.3f}        | {roc_auc_score(yc_te, y_scores_sgd):.3f}\")\n",
        "\n",
        "# All three linear models should yield similar (and high) performance\n",
        "# because the data are linearly separable.\n"
      ],
      "metadata": {
        "id": "zFpN5kSVfbOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When Linear Classifiers Fail (The XOR Problem)\n",
        "\n",
        "Our Perceptron (as well as Logistic Regression and linear SVM) worked well because the data were **linearly separable** — a single straight line was sufficient to separate the classes.\n",
        "\n",
        "But what happens when the data are **not** linearly separable?\n",
        "\n",
        "The most famous example is the **XOR problem** (Exclusive OR).  \n",
        "In this scenario, data points are arranged in a “cross” pattern.\n",
        "\n",
        "No matter how hard you try, **you cannot draw a *single straight line*** that correctly separates the blue points from the orange points.\n",
        "\n",
        "Linear models fail completely on this task. Their accuracy will be around **50%**, which is no better than random guessing.\n",
        "\n",
        "Historically, this failure was a major disappointment and nearly **halted neural network research for decades** (since the Perceptron is a single-layer neural network).\n",
        "\n",
        "To solve XOR, we need **non-linear models**, such as:\n",
        "\n",
        "* SVM with a non-linear kernel (e.g., RBF)\n",
        "* Random Forest (decision trees)\n",
        "* **Multi-Layer Perceptrons (MLPs)** — the basis of modern deep learning, capable of learning non-linear decision boundaries\n"
      ],
      "metadata": {
        "id": "jHdgAH5IgCyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Failure of a Linear Model on Non-Linear Data\n",
        "\n",
        "# For demonstration, we use the \"make_moons\" dataset.\n",
        "# It generates two interleaving moon-shaped clusters that cannot be separated by a straight line.\n",
        "\n",
        "# 1. Create non-linear data\n",
        "X_nl, y_nl = make_moons(n_samples=300, noise=0.15, random_state=RSTATE)\n",
        "\n",
        "# 2. Create a linear model\n",
        "# We use a Pipeline with an SGD Perceptron\n",
        "pipe_fail = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('model', SGDClassifier(loss='perceptron', max_iter=1000, random_state=RSTATE))\n",
        "])\n",
        "\n",
        "# 3. Train\n",
        "pipe_fail.fit(X_nl, y_nl)\n",
        "\n",
        "# 4. Evaluate\n",
        "y_pred_fail = pipe_fail.predict(X_nl)\n",
        "acc_fail = accuracy_score(y_nl, y_pred_fail)\n",
        "\n",
        "print(f\"--- Performance on non-linear data (make_moons) ---\")\n",
        "print(f\"Accuracy: {acc_fail:.3f}\")\n",
        "print(\"The model performs only slightly better than random guessing (50%)...\")\n",
        "\n",
        "\n",
        "# 5. Visualize the failure (same logic as in an earlier cell)\n",
        "x0_min, x0_max = X_nl[:, 0].min() - 0.5, X_nl[:, 0].max() + 0.5\n",
        "x1_min, x1_max = X_nl[:, 1].min() - 0.5, X_nl[:, 1].max() + 0.5\n",
        "xx0, xx1 = np.meshgrid(np.linspace(x0_min, x0_max, 200),\n",
        "                       np.linspace(x1_min, x1_max, 200))\n",
        "grid_points = np.c_[xx0.ravel(), xx1.ravel()]\n",
        "\n",
        "# Get predictions from the trained pipeline\n",
        "zz = pipe_fail.predict(grid_points).reshape(xx0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx0, xx1, zz, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X_nl[:, 0], X_nl[:, 1], c=y_nl,\n",
        "            cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "\n",
        "plt.title(f\"Failure of a Linear Model (Accuracy: {acc_fail:.3f})\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "# The plot clearly shows that a single straight line cannot separate these data.\n"
      ],
      "metadata": {
        "id": "fOmNn8EofsxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FV1v-DpDgCO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}