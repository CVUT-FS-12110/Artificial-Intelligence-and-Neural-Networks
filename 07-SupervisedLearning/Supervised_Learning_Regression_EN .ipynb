{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eX1CQqID90yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "RSTATE = 42"
      ],
      "metadata": {
        "id": "GPDFyMPWA_8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning — Linear Regression\n",
        "\n",
        "## 1. What is Regression?\n",
        "\n",
        "Regression is a fundamental task in **supervised learning**. Its goal is to **predict a continuous numerical value**.\n",
        "\n",
        "Unlike *classification*, which predicts a category (e.g., “dog” or “cat”, `0` or `1`), regression predicts a **numeric value** (e.g., 150,000 CZK, 25.5 °C).\n",
        "\n",
        "We aim to find a **mathematical function (model)** that best describes the relationship between **input features** and a **target variable**.\n",
        "\n",
        "* **Example:**\n",
        "    * **Features (X):** `apartment size`, `number of rooms`, `distance from city center`\n",
        "    * **Target (y):** `apartment price`\n",
        "\n",
        "In this exercise, we focus on **linear regression**. It assumes that the relationship is linear — meaning it can be described by a **straight line** (in one dimension) or a **hyperplane** (in multiple dimensions).\n",
        "\n",
        "The goal is to find such parameters (weights) of this line so that it is “as close as possible” to all training data points. This process is called training the model.\n"
      ],
      "metadata": {
        "id": "fvhE9xTS95pA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Background\n",
        "\n",
        "### Linear regression\n",
        "\n",
        "The model attempts to fit a straight line (or a hyperplane in higher dimensions) through the data.\n",
        "\n",
        "Prediction (hypothesis):\n",
        "$$\n",
        "\\hat{y}_i = w^\\top x_i + b\n",
        "$$\n",
        "where \\( w \\) are the weights (slope of the line) and \\( b \\) is the bias (y-intercept).\n",
        "\n",
        "The loss function we minimize is the **Mean Squared Error (MSE)**:\n",
        "$$\n",
        "\\mathcal{L}_{\\mathrm{MSE}}(w,b) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "This function simply says: “Sum the squared distances between each prediction \\( \\hat{y}_i \\) and the actual value \\( y_i \\), and divide by the number of samples.”\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Our goal is to find parameters \\( w \\) and \\( b \\) that minimize \\( \\mathcal{L}_{\\mathrm{MSE}} \\). We do this using **gradient descent**.\n",
        "\n",
        "It is an iterative process where we adjust \\( w \\) and \\( b \\) “step by step” in the direction that reduces the error the most. This direction is given by the negative gradient (partial derivatives).\n",
        "\n",
        "Gradients for **basic MSE**:\n",
        "$$\n",
        "\\nabla_w \\mathcal{L} = \\frac{2}{n} X^\\top (Xw + b\\mathbf{1} - y)\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
        "$$\n",
        "\n",
        "Parameter updates are performed repeatedly as:\n",
        "$$\n",
        "w \\leftarrow w - \\eta \\nabla_w \\mathcal{L}\n",
        "$$\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
        "$$\n",
        "\n",
        "where \\( \\eta \\) (eta) is the **learning rate**, which controls the step size taken in each iteration.\n",
        "\n",
        "---\n",
        "\n",
        "### Improvement: Why Regularization? (Preventing Overfitting)\n",
        "\n",
        "When we have many features, or when some features are highly correlated (multicollinearity), the model may **overfit**.\n",
        "\n",
        "This means it fits the training data extremely well but assigns **very large weights** to some features. As a result, the model becomes sensitive to small changes in the input and **fails to generalize** to new, unseen data (e.g., validation or test sets).\n",
        "\n",
        "**L2 regularization (Ridge regression)** helps prevent this by adding a penalty term that discourages large weights.\n",
        "\n",
        "Loss function for **Ridge Regression** (MSE + L2 penalty):\n",
        "$$\n",
        "\\mathcal{L}_{\\mathrm{Ridge}} = \\mathcal{L}_{\\mathrm{MSE}} + \\alpha \\|w\\|_2^2\n",
        "= \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 + \\alpha \\|w\\|_2^2\n",
        "$$\n",
        "\n",
        "* \\( \\alpha \\) (alpha) is a hyperparameter that controls the strength of regularization\n",
        "* \\( \\|w\\|_2^2 \\) is the squared L2 norm of the weight vector (sum of squared weights)\n",
        "\n",
        "The gradient changes only for \\( w \\) (bias \\( b \\) is typically not regularized):\n",
        "\n",
        "$$\n",
        "\\nabla_w \\mathcal{L}_{\\mathrm{Ridge}}\n",
        "= \\underbrace{\\frac{2}{n} X^\\top (Xw + b\\mathbf{1} - y)}_{\\text{original gradient}}\n",
        "+ \\underbrace{2 \\alpha w}_{\\text{regularization term}}\n",
        "$$\n",
        "\n",
        "Thus, at each gradient-descent step, we both decrease the MSE and “pull” the weights toward zero (known as *weight decay*)."
      ],
      "metadata": {
        "id": "21fCe9MQ9_2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization Methods: Batch vs. Stochastic vs. Mini-Batch GD\n",
        "\n",
        "Our `LinearRegressorGD` uses **Batch Gradient Descent**, because it is the easiest method to understand conceptually. However, it is not the only method — and in practice, not even the most common one.\n",
        "\n",
        "There are three main strategies for computing gradients and updating model weights:\n",
        "\n",
        "### 1. Batch Gradient Descent (what we use)\n",
        "In each epoch, the gradient (the direction of steepest error decrease) is computed from the **entire training dataset** at once:\n",
        "$$\n",
        "\\nabla_w \\mathcal{L} = \\frac{2}{n} \\sum_{i=1}^n (X_i^\\top (\\hat{y}_i - y_i)) + 2 \\alpha w\n",
        "$$\n",
        "\n",
        "Then we make **a single update** to the weights:\n",
        "\n",
        "\\[\n",
        "w \\leftarrow w - \\eta \\nabla_w \\mathcal{L}\n",
        "\\]\n",
        "\n",
        "* **Advantages:**\n",
        "  * Very stable and smooth convergence.\n",
        "  * Guarantees finding the global minimum (for convex functions like MSE).\n",
        "\n",
        "* **Disadvantages:**\n",
        "  * **Extremely slow** for large datasets (e.g., 1M samples) — it must process *all* data before taking one step.\n",
        "  * High memory requirements (entire dataset must fit into RAM).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Stochastic Gradient Descent (SGD)\n",
        "In each epoch, the training data are shuffled. Then we process **one sample at a time**, computing the gradient and updating weights **after each individual sample**.\n",
        "\n",
        "If the dataset has \\(n\\) samples, we perform \\(n\\) small weight updates per epoch:\n",
        "\n",
        "$$\n",
        "\\nabla_w \\mathcal{L}_i = 2 (X_i^\\top (\\hat{y}_i - y_i)) + 2 \\alpha w \\quad \\text{(gradient for a single sample } i)\n",
        "$$\n",
        "\n",
        "\\[\n",
        "w \\leftarrow w - \\eta \\nabla_w \\mathcal{L}_i\n",
        "\\]\n",
        "\n",
        "* **Advantages:**\n",
        "  * **Very fast** iterations — the model starts learning immediately.\n",
        "  * Low memory usage (one sample at a time).\n",
        "  * Noise in gradients (from individual samples) can help escape shallow local minima.\n",
        "\n",
        "* **Disadvantages:**\n",
        "  * Convergence is noisy and unstable; weights oscillate around the optimum.\n",
        "  * Usually does not fully converge — often requires gradually decreasing the learning rate \\((\\eta)\\).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Mini-Batch Gradient Descent (the “golden mean”)\n",
        "This is a compromise and the **most widely used method in practice** (especially in deep learning).\n",
        "\n",
        "The dataset is split into small \"batches\" (**mini-batches**) of size \\(k\\) (e.g., \\(k = 32\\), 64, or 256 samples). Gradients are computed and weights updated after each batch.\n",
        "\n",
        "* **Advantages:**\n",
        "  * Much faster than Batch GD.\n",
        "  * More stable convergence than SGD.\n",
        "  * Fully benefits from vectorization and modern GPUs.\n",
        "\n",
        "* **Disadvantages:**\n",
        "  * Introduces another hyperparameter (`batch_size`).\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion:**  \n",
        "In this notebook, we stick with **Batch GD** for clarity and stability. In the classification notebook, we will explore a method whose update pattern (sample-by-sample) is much closer to **SGD**.\n"
      ],
      "metadata": {
        "id": "67N8rF5YAJ8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "# We use the 'make_regression' function from scikit-learn\n",
        "# n_features=1: Only one feature (X), so we can easily visualize it\n",
        "# noise=18.0: Amount of noise; the higher it is, the harder the task becomes\n",
        "X_reg, y_reg = make_regression(n_samples=350, n_features=1, noise=18.0, random_state=RSTATE)\n",
        "\n",
        "# 2. Split into Train (60%), Validation (20%), Test (20%)\n",
        "# This is a key step for proper model evaluation.\n",
        "# We train the model on 'Train', tune hyperparameters on 'Validation',\n",
        "# and measure final performance on 'Test' (which the model has never seen).\n",
        "\n",
        "# Step 1: Split into Train (60%) and \"Temporary\" (40%)\n",
        "Xr_tr, Xr_tmp, yr_tr, yr_tmp = train_test_split(X_reg, y_reg, test_size=0.4, random_state=RSTATE)\n",
        "\n",
        "# Step 2: Split \"Temporary\" (40%) into Validation (20%) and Test (20%)\n",
        "# (test_size=0.5 means 50% of 40%, which equals 20% of the full dataset)\n",
        "Xr_va, Xr_te, yr_va, yr_te = train_test_split(Xr_tmp, yr_tmp, test_size=0.5, random_state=RSTATE)\n",
        "\n",
        "print(\"--- Dataset shapes (before scaling) ---\")\n",
        "print(f\"Train:      {Xr_tr.shape}, {yr_tr.shape}\")\n",
        "print(f\"Validation: {Xr_va.shape}, {yr_va.shape}\")\n",
        "print(f\"Test:       {Xr_te.shape}, {yr_te.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# 3. Standardization (Scaling) of data\n",
        "# Why? Gradient Descent (GD) and L2 regularization work much better\n",
        "# and faster when all features have a similar scale.\n",
        "# Standardization transforms data to have mean 0 and standard deviation 1.\n",
        "\n",
        "# IMPORTANT: Preventing Data Leakage\n",
        "# The scaler (StandardScaler) must be 'fit' (learn mean & std)\n",
        "# ONLY on training data (Xr_tr).\n",
        "sc_r = StandardScaler().fit(Xr_tr)\n",
        "\n",
        "# The trained scaler is then ONLY APPLIED to validation and test data\n",
        "Xr_tr_s = sc_r.transform(Xr_tr)\n",
        "Xr_va_s = sc_r.transform(Xr_va)\n",
        "Xr_te_s = sc_r.transform(Xr_te)\n",
        "\n",
        "# The target variable (y) usually does not need to be scaled for linear regression.\n",
        "\n",
        "print(\"--- Scaling check (mean / std dev) ---\")\n",
        "print(f\"Train (scaled):      {Xr_tr_s.mean():.2f} / {Xr_tr_s.std():.2f}\")\n",
        "print(f\"Validation (scaled): {Xr_va_s.mean():.2f} / {Xr_va_s.std():.2f}\")\n",
        "print(f\"Test (scaled):       {Xr_te_s.mean():.2f} / {Xr_te_s.std():.2f}\")\n"
      ],
      "metadata": {
        "id": "vS64708Z9_HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the data we generated.\n",
        "# We will plot the TRAINING set (before scaling, so the axes remain readable).\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(Xr_tr, yr_tr, alpha=0.7, label=\"Training data\")\n",
        "plt.xlabel(\"Feature X\")\n",
        "plt.ylabel(\"Target value y\")\n",
        "plt.title(\"Generated regression data (Train set)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O3pb0hQ3A6RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressorGD:\n",
        "    \"\"\"\n",
        "    Implementation of linear regression.\n",
        "\n",
        "    Trained using Batch Gradient Descent (GD).\n",
        "    Includes L2 (Ridge) regularization and Early Stopping.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr: float = 0.05, n_epochs: int = 1500,\n",
        "                 fit_intercept: bool = True, alpha: float = 0.0,\n",
        "                 early_stopping_patience: int = 20):\n",
        "        \"\"\"\n",
        "        Initialize model hyperparameters.\n",
        "\n",
        "        lr: Learning rate\n",
        "        n_epochs: Maximum number of epochs (passes over data)\n",
        "        fit_intercept: Whether to learn 'b' (bias/intercept)\n",
        "        alpha: Strength of L2 regularization (0 = no regularization)\n",
        "        early_stopping_patience: Number of epochs without improvement on\n",
        "                                 the validation set before training stops.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.alpha = alpha  # L2 regularization strength\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "\n",
        "        # Internal variables for learned parameters and history\n",
        "        self.w = None\n",
        "        self.b = 0.0\n",
        "        self.train_rmse_history = []\n",
        "        self.val_rmse_history = []\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray,\n",
        "            X_val: np.ndarray = None, y_val: np.ndarray = None):\n",
        "        \"\"\"\n",
        "        Train the model on (X, y), optionally using (X_val, y_val)\n",
        "        for early stopping.\n",
        "        \"\"\"\n",
        "        n, d = X.shape  # n = number of samples, d = number of features\n",
        "        self.w = np.zeros(d, dtype=float)\n",
        "        self.b = 0.0\n",
        "\n",
        "        # --- Early Stopping setup ---\n",
        "        use_es = X_val is not None and y_val is not None\n",
        "        best_val_rmse = np.inf\n",
        "        patience_counter = 0\n",
        "        best_w = self.w.copy()\n",
        "        best_b = self.b\n",
        "        # ----------------------------\n",
        "\n",
        "        print(f\"Training starts (max {self.n_epochs} epochs, L2 alpha={self.alpha})...\")\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # 1. Predictions (forward pass)\n",
        "            y_hat = X @ self.w + (self.b if self.fit_intercept else 0.0)\n",
        "\n",
        "            # 2. Compute error (residuals)\n",
        "            residual = (y_hat - y)\n",
        "\n",
        "            # 3. Compute gradients (Batch GD — from all samples at once)\n",
        "\n",
        "            # Gradient for weights 'w' with L2 regularization\n",
        "            # (2.0 / n) * (X.T @ residual)  <- gradient from MSE\n",
        "            # + 2.0 * self.alpha * self.w   <- gradient from L2 penalty\n",
        "            grad_w = (2.0 / n) * (X.T @ residual) + 2.0 * self.alpha * self.w\n",
        "\n",
        "            grad_b = 0.0\n",
        "            if self.fit_intercept:\n",
        "                # Gradient for bias 'b' (bias is typically not regularized)\n",
        "                grad_b = (2.0 / n) * np.sum(residual)\n",
        "\n",
        "            # 4. Weight update (step against the gradient direction)\n",
        "            self.w -= self.lr * grad_w\n",
        "            if self.fit_intercept:\n",
        "                self.b -= self.lr * grad_b\n",
        "\n",
        "            # --- Log history and check Early Stopping ---\n",
        "            train_rmse = np.sqrt(mean_squared_error(y, y_hat))\n",
        "            self.train_rmse_history.append(train_rmse)\n",
        "\n",
        "            if use_es:\n",
        "                val_rmse = np.sqrt(mean_squared_error(y_val, self.predict(X_val)))\n",
        "                self.val_rmse_history.append(val_rmse)\n",
        "\n",
        "                # Save the best model according to validation error\n",
        "                if val_rmse < best_val_rmse - 1e-5:  # Small tolerance for improvement\n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_w = self.w.copy()\n",
        "                    best_b = self.b\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                # If the model has not improved for 'patience' epochs, stop training\n",
        "                if patience_counter >= self.early_stopping_patience:\n",
        "                    print(f\"INFO: Early stopping at epoch {epoch + 1} (Val RMSE: {best_val_rmse:.4f})\")\n",
        "                    break\n",
        "\n",
        "            # Print progress every N epochs\n",
        "            if (epoch + 1) % (self.n_epochs // 10) == 0 and not use_es:\n",
        "                print(f\"  Epoch {epoch+1}/{self.n_epochs}, Train RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "        # --- End of training ---\n",
        "        if use_es:\n",
        "            # Restore the best weights found on the validation set\n",
        "            print(f\"INFO: Training finished. Restoring model from epoch {epoch + 1 - patience_counter}.\")\n",
        "            self.w = best_w\n",
        "            self.b = best_b\n",
        "        else:\n",
        "            print(\"INFO: Training finished (reached max epochs).\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions on new data X.\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise RuntimeError(\"Model is not trained. Call 'fit' first.\")\n",
        "\n",
        "        return X @ self.w + (self.b if self.fit_intercept else 0.0)\n"
      ],
      "metadata": {
        "id": "99PfrZ34BS1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper functions for advanced metrics ---\n",
        "def adjusted_r2(r2, n, p):\n",
        "    \"\"\"\n",
        "    Computes Adjusted R^2.\n",
        "    Penalizes the model for adding features (p) that do not provide benefit.\n",
        "    n = number of samples, p = number of features\n",
        "    \"\"\"\n",
        "    return 1.0 - (1.0 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "def mape(y_true, y_pred, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Mean Absolute Percentage Error (MAPE).\n",
        "    Reports the average error in percent. It is sensitive to zero values in y_true.\n",
        "    \"\"\"\n",
        "    mask = np.abs(y_true) > eps\n",
        "    if not np.any(mask):\n",
        "        return np.nan\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "\n",
        "# --- 1. Model initialization ---\n",
        "# Set a high n_epochs (e.g., 5000); Early Stopping will likely halt earlier.\n",
        "# alpha=0.01: Use mild L2 regularization.\n",
        "# early_stopping_patience=50: Stop training if Val RMSE does not improve for 50 consecutive epochs.\n",
        "reg = LinearRegressorGD(\n",
        "    lr=0.05,\n",
        "    n_epochs=5000,\n",
        "    alpha=0.01,\n",
        "    early_stopping_patience=50\n",
        ")\n",
        "\n",
        "# --- 2. Model training ---\n",
        "# Train on scaled training data (Xr_tr_s, yr_tr)\n",
        "# and use scaled validation data (Xr_va_s, yr_va) for Early Stopping.\n",
        "reg.fit(Xr_tr_s, yr_tr, X_val=Xr_va_s, y_val=yr_va)\n",
        "\n",
        "# --- 3. Predictions on all splits ---\n",
        "# (Use the learned model on scaled data)\n",
        "yr_pred_tr = reg.predict(Xr_tr_s)\n",
        "yr_pred_va = reg.predict(Xr_va_s)\n",
        "yr_pred_te = reg.predict(Xr_te_s)\n",
        "\n",
        "# --- 4. Evaluation on the TEST set ---\n",
        "# These are the final numbers we report for model performance.\n",
        "print(\"\\n\" + \"---\" * 15)\n",
        "print(\"Final performance on the TEST set:\")\n",
        "print(\"---\" * 15)\n",
        "\n",
        "n_test = yr_te.shape[0]\n",
        "p_features = Xr_te_s.shape[1]\n",
        "\n",
        "mse  = mean_squared_error(yr_te, yr_pred_te)\n",
        "rmse = np.sqrt(mse)\n",
        "mae  = mean_absolute_error(yr_te, yr_pred_te)\n",
        "r2   = r2_score(yr_te, yr_pred_te)\n",
        "adjr2 = adjusted_r2(r2, n=n_test, p=p_features)\n",
        "mape_v = mape(yr_te, yr_pred_te)\n",
        "\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.3f}\")\n",
        "print(f\"MAE (Mean Absolute Error):      {mae:.3f}\")\n",
        "print(f\"MAPE (Mean Absolute % Error):   {mape_v:.2f} %\")\n",
        "print(f\"R² (R-squared):                 {r2:.3f}\")\n",
        "print(f\"Adjusted R²:                    {adjr2:.3f}\")\n",
        "\n",
        "# It is also good practice to check the mean error (model bias)\n",
        "me = float(np.mean(yr_pred_te - yr_te))\n",
        "print(f\"ME (Mean Error / Bias):         {me:.3f}\")\n"
      ],
      "metadata": {
        "id": "822ryvAwBlaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Metrics (Model Diagnostics)\n",
        "\n",
        "A single number (such as RMSE) never tells the whole story. It is important to look at multiple metrics, because each one measures a different aspect of model performance.\n",
        "\n",
        "$$\\phantom{}$$\n",
        "\n",
        "* **RMSE (Root Mean Squared Error)**\n",
        "    * **What it measures:** “What is the *typical* error of my model, in the same units as the target \\(y\\)?”\n",
        "      (For example, if the target is in CZK, RMSE is also in CZK.)\n",
        "    * **Interpretation:** Lower is better.\n",
        "    * **Property:** Since the errors are *squared*, RMSE is **very sensitive to outliers**. A single very bad prediction increases RMSE much more than it would increase MAE.\n",
        "\n",
        "* **MAE (Mean Absolute Error)**\n",
        "    * **What it measures:** The *average* error of the model, in the same units as the target.\n",
        "    * **Interpretation:** Lower is better.\n",
        "    * **Property:** Uses absolute values, so it is **less sensitive to outliers** than RMSE.\n",
        "    * **Tip:** If **RMSE is much larger than MAE**, your model likely has a few very large errors (probably due to outliers).\n",
        "    *(diagram: RMSE vs MAE effect of outliers)*\n",
        "\n",
        "* **MAPE (Mean Absolute Percentage Error)**\n",
        "    * **What it measures:** “On average, by how many *percent* does the model miss the true value?”\n",
        "    * **Interpretation:** Lower is better. Great metric for business communication (e.g., “Our price model has a 5% average error.”)\n",
        "    * **Property:** Can be problematic when target values \\(y\\) are zero or close to zero (division by zero issues).\n",
        "\n",
        "* **R² (R-squared / Coefficient of Determination)**\n",
        "    * **What it measures:** What *percentage of variability* in \\(y\\) the model can explain.\n",
        "    * **Interpretation:**\n",
        "        * **1.0:** Perfect model\n",
        "        * **0.7:** Model explains 70% of the variance\n",
        "        * **0.0:** Model is no better than predicting the mean of \\(y\\)\n",
        "        * *(can even be negative if the model is worse than always predicting the mean)*\n",
        "    * **Property:** R² **never decreases** when you add features — even if they are useless.\n",
        "\n",
        "* **Adjusted R²**\n",
        "    * **What it measures:** A modified version of R² that **penalizes adding unnecessary features** that do not actually improve the model.\n",
        "    * **Interpretation:** Always ≤ R². Much more reliable when comparing models with *different numbers of features*.\n",
        "\n",
        "* **ME (Mean Error / Bias)**\n",
        "    * **What it measures:** “Does my model systematically *overestimate* or *underestimate*?”\n",
        "    * **Interpretation:** Unlike MAE, errors do not use absolute values, so positive and negative errors **cancel each other out**.\n",
        "        * **ME > 0:** Model tends to overpredict\n",
        "        * **ME < 0:** Model tends to underpredict\n",
        "    * **Property:** We want this value to be as **close to zero** as possible. A non-zero ME indicates systematic bias in the model.\n"
      ],
      "metadata": {
        "id": "OTeYuiQDD5Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training History (RMSE vs. Epoch)\n",
        "\n",
        "This plot is crucial for diagnosing the learning process itself. It shows how the model error changed with each epoch (pass through the data).\n",
        "\n",
        "* **Train RMSE (blue line):** Shows how well the model fits the data it is trained on. We expect this curve to generally decrease.\n",
        "* **Validation RMSE (orange line):** Shows how well the model **generalizes** to unseen data. This is the most important curve for model evaluation.\n",
        "\n",
        "**What we look for in the plot:**\n",
        "\n",
        "* **Ideal scenario:** Both curves decrease and stabilize at a low level close to each other.\n",
        "* **Overfitting:** The blue (Train) curve drops low, but the orange (Validation) curve starts rising again at some point.  \n",
        "  The model “memorizes” the training data but loses its ability to generalize.\n",
        "* **Underfitting:** Both curves stay high. The model is too simple and fails to capture even the underlying trend in the training data.\n",
        "* **Early Stopping marker (red line):** Indicates the epoch where training stopped.  \n",
        "  This is the point with the lowest validation error. Without early stopping, the model would begin to overfit (the orange curve would gradually rise).\n"
      ],
      "metadata": {
        "id": "ylv1mED4E2GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve training error history from the trained model\n",
        "train_history = reg.train_rmse_history\n",
        "val_history = reg.val_rmse_history\n",
        "\n",
        "# Find the epoch where the validation RMSE was the lowest\n",
        "best_epoch = np.argmin(val_history)\n",
        "best_val_rmse = val_history[best_epoch]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(train_history, label=\"Train RMSE\", lw=2)\n",
        "plt.plot(val_history, label=\"Validation RMSE\", lw=2)\n",
        "\n",
        "# Mark the best epoch from which the model was restored\n",
        "plt.axvline(best_epoch, color='red', linestyle='--',\n",
        "            label=f\"Best model (Epoch: {best_epoch}, RMSE: {best_val_rmse:.3f})\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE (Root Mean Squared Error)\")\n",
        "plt.title(\"Training History (Early Stopping)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Trim the y-axis to highlight differences (ignore the first ~5–10 epochs)\n",
        "plt.ylim(bottom=0, top=np.max(val_history[5:]) * 1.1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hnVJfxc0CjOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model vs. Test Data\n",
        "\n",
        "This plot shows the final outcome of our work.\n",
        "\n",
        "* **Blue points:** These are the **test data** (`Xr_te_s`, `yr_te`). They are the real (“ground truth”) values that the model **never saw** during training or validation.\n",
        "* **Red line:** This is our trained model — its **predictions**. It shows what output \\(y\\) the model would predict for any value of \\(x\\).\n",
        "* **Light-red band:** This is a heuristic band of \\(\\pm 2 \\times \\text{RMSE}\\). If the errors are normally distributed, we would expect roughly **95% of all test points** to lie within this band.\n",
        "\n",
        "**What we look for:**\n",
        "We want the red line to cut through the cloud of blue points as well as possible, and for the points to be as close to this line as possible.\n"
      ],
      "metadata": {
        "id": "uq1Yaaq_E68z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression line vs. Test data\n",
        "\n",
        "# Create an X-axis for a smooth line\n",
        "# (we must use the standardized space in which the model was trained)\n",
        "min_x_s = Xr_te_s.min()\n",
        "max_x_s = Xr_te_s.max()\n",
        "# Extend the range by 10% for a nicer plot\n",
        "padding = (max_x_s - min_x_s) * 0.1\n",
        "xs_std = np.linspace(min_x_s - padding, max_x_s + padding, 200).reshape(-1, 1)\n",
        "\n",
        "# Compute predictions for this line\n",
        "ys_pred_line = reg.predict(xs_std)\n",
        "\n",
        "# Reload RMSE from cell 9 for plotting the band\n",
        "# rmse = np.sqrt(mean_squared_error(yr_te, yr_pred_te))\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# 1. Plot test data (ground truth)\n",
        "plt.scatter(Xr_te_s, yr_te, alpha=0.7, label=\"Test data (ground truth)\")\n",
        "\n",
        "# 2. Plot our learned line\n",
        "plt.plot(xs_std, ys_pred_line, color='red', lw=3, label=\"Regression line (prediction)\")\n",
        "\n",
        "# 3. (Optional) Plot the heuristic ±2*RMSE band\n",
        "# This band should contain ~95% of all points\n",
        "plt.fill_between(xs_std.ravel(), ys_pred_line - 2*rmse, ys_pred_line + 2*rmse,\n",
        "                 color='red', alpha=0.1, label=f\"Heuristic band (±2×RMSE ≈ ±{2*rmse:.2f})\")\n",
        "\n",
        "plt.xlabel(\"X (standardized)\")\n",
        "plt.ylabel(\"y (target value)\")\n",
        "plt.title(\"Regression: Final model vs. Test set\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nTKQP6_TCjkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Analysis (Errors vs. Predictions)\n",
        "\n",
        "This is the most important diagnostic plot for regression.  \n",
        "A **residual** is simply another name for the model error (`actual − prediction`).\n",
        "\n",
        "**What we want to see:**  \n",
        "A “random cloud of points” centered around the zero line.\n",
        "\n",
        "**What we do NOT want to see:** Any visible pattern.  \n",
        "A pattern means there is still structure in the data that the model failed to capture. Examples of problematic patterns:\n",
        "\n",
        "* **Trumpet / funnel shape (heteroscedasticity):**  \n",
        "  The prediction error increases as predictions get larger.  \n",
        "  The model is more certain for smaller values and worse for larger values.\n",
        "\n",
        "* **Arc / parabola:**  \n",
        "  The model systematically misses a nonlinear relationship.\n"
      ],
      "metadata": {
        "id": "9qTwPJeUERjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual analysis (Errors vs. Predictions)\n",
        "\n",
        "# Residuals = (actual value - predicted value)\n",
        "resid = yr_te - yr_pred_te\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# X-axis: predicted value\n",
        "# Y-axis: residual (error)\n",
        "plt.scatter(yr_pred_te, resid, alpha=0.7, edgecolors='k', s=60)\n",
        "\n",
        "# Add a zero line — residuals should be centered around it\n",
        "plt.axhline(0.0, color='red', linestyle='--', lw=2)\n",
        "\n",
        "plt.xlabel(\"Predicted value (y_hat)\")\n",
        "plt.ylabel(\"Residual (y - y_hat)\")\n",
        "plt.title(\"Residual Analysis (Test set)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aE6JRzjJCqqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QQ Plot of Residuals\n",
        "\n",
        "This plot checks whether our errors (residuals) are **normally distributed**.  \n",
        "This is an important assumption for many statistical tests (although it is not strictly required for training the model itself).\n",
        "\n",
        "**What we want to see:**  \n",
        "The points should lie as close as possible to the red line.\n",
        "\n",
        "* If the points deviate at the ends (“tails”), it means we have more large errors (outliers) than would be expected under a normal distribution.\n"
      ],
      "metadata": {
        "id": "qgA7k7rnEYrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QQ Plot of residuals (Test set)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "# stats.probplot compares the distribution of our data (resid)\n",
        "# with a theoretical normal distribution (\"norm\")\n",
        "stats.probplot(resid, dist=\"norm\", plot=plt)\n",
        "plt.title(\"QQ Plot of Residuals (Normality Check)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZA8vnrykCu6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross-Validation\n",
        "\n",
        "A simple split into `train`, `validation`, and `test` sets is useful, but it has one drawback: the result (e.g., RMSE on the validation set) heavily depends on which data *randomly* ended up in each set.  \n",
        "What if we are unlucky and the validation set contains only “difficult” examples? Then our performance estimate would be unnecessarily pessimistic.\n",
        "\n",
        "**K-Fold Cross-Validation** solves this problem neatly:\n",
        "\n",
        "1. The entire training dataset is split into \\(K\\) (e.g., 5 or 10) equally sized parts (“folds”).\n",
        "2. The process is repeated \\(K\\) times:\n",
        "   * One fold becomes the **validation set**\n",
        "   * The remaining \\(K-1\\) folds are used as the **training set**\n",
        "   * The model is trained and evaluated\n",
        "3. This results in \\(K\\) different performance estimates (e.g., \\(K\\) values of RMSE)\n",
        "4. These values are averaged to obtain a much more stable and reliable estimate of model performance.\n",
        "\n",
        "**Key rule (preventing data leakage):**  \n",
        "The scaler (or any other data transformation) must **always be fit again inside each fold** using only the training data of that fold.\n"
      ],
      "metadata": {
        "id": "GqjQFSwXEft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of 5-Fold Cross-Validation\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# For demonstrating CV, use the original unscaled data\n",
        "# (combine training and validation sets; leave the test set aside)\n",
        "X_cv_data = np.vstack((Xr_tr, Xr_va))\n",
        "y_cv_data = np.concatenate((yr_tr, yr_va))\n",
        "\n",
        "# Initialize the K-Fold splitter\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=RSTATE)\n",
        "\n",
        "cv_rmse_scores = []\n",
        "fold_counter = 1\n",
        "\n",
        "print(\"Starting 5-Fold Cross-Validation...\")\n",
        "# Loop over 5 folds\n",
        "for tr_idx, va_idx in kf.split(X_cv_data):\n",
        "    # 1. Split data into the current fold\n",
        "    Xtr_fold, Xva_fold = X_cv_data[tr_idx], X_cv_data[va_idx]\n",
        "    ytr_fold, yva_fold = y_cv_data[tr_idx], y_cv_data[va_idx]\n",
        "\n",
        "    # 2. FIT THE SCALER ONLY ON THE TRAINING SPLIT OF THE FOLD\n",
        "    sc_fold = StandardScaler().fit(Xtr_fold)\n",
        "\n",
        "    # 3. Transform both splits using the fitted scaler\n",
        "    Xtr_s_fold = sc_fold.transform(Xtr_fold)\n",
        "    Xva_s_fold = sc_fold.transform(Xva_fold)\n",
        "\n",
        "    # 4. Train the model (without Early Stopping, for a fixed number of epochs)\n",
        "    #    Early Stopping doesn't make sense here because there is no separate validation set\n",
        "    mdl_fold = LinearRegressorGD(lr=0.05, n_epochs=500, alpha=0.01)\n",
        "    mdl_fold.fit(Xtr_s_fold, ytr_fold)  # Train without a validation split\n",
        "\n",
        "    # 5. Evaluate on the validation fold\n",
        "    y_pred_fold = mdl_fold.predict(Xva_s_fold)\n",
        "    rmse_fold = np.sqrt(mean_squared_error(yva_fold, y_pred_fold))\n",
        "\n",
        "    print(f\"  Fold {fold_counter}/5 RMSE: {rmse_fold:.3f}\")\n",
        "    cv_rmse_scores.append(rmse_fold)\n",
        "    fold_counter += 1\n",
        "\n",
        "# --- Final CV result ---\n",
        "print(\"---\" * 10)\n",
        "print(\"K-Fold Cross-Validation result (RMSE):\")\n",
        "print(f\"  Mean RMSE: {np.mean(cv_rmse_scores):.3f}\")\n",
        "print(f\"  Std. dev. of RMSE: {np.std(cv_rmse_scores):.3f}\")\n",
        "\n",
        "# This average result is a much more reliable performance estimate\n",
        "# than RMSE from a single validation split.\n"
      ],
      "metadata": {
        "id": "iopfcxKzC2hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models from Scikit-learn\n",
        "\n",
        "Our `LinearRegressorGD` is great for understanding how things work “under the hood.”  \n",
        "In practice, however, we almost always use optimized and robust implementations from the `scikit-learn` library.\n",
        "\n",
        "The most common linear regression models in `sklearn`:\n",
        "\n",
        "* **`LinearRegression`**\n",
        "    * **What it is:** Standard linear regression.\n",
        "    * **How it works:** Instead of Gradient Descent, it uses a closed-form analytical solution (the **Normal Equation**). Very fast for small and medium-sized datasets.\n",
        "    * **Equivalent to:** Our model with `alpha=0` (but solved analytically, not iteratively).\n",
        "\n",
        "* **`Ridge`**\n",
        "    * **What it is:** Linear regression with **L2 regularization**.\n",
        "    * **How it works:** Also uses an analytical solution, adjusted with the L2 penalty term. Preferred when you have many features or risk of multicollinearity.\n",
        "    * **Equivalent to:** Our `LinearRegressorGD` with `alpha > 0`.\n",
        "\n",
        "* **`Lasso`**\n",
        "    * **What it is:** Linear regression with **L1 regularization**  \n",
        "      \\(\\alpha \\sum |w_i|\\)\n",
        "    * **How it works:** Unlike L2, L1 can “zero out” weights of irrelevant features.\n",
        "      Excellent for **automatic feature selection**.\n",
        "    * **Equivalent to:** Conceptually similar, but uses a different penalty (L1).\n",
        "\n",
        "* **`ElasticNet`**\n",
        "    * **What it is:** A combination of L1 and L2 regularization.  \n",
        "      Takes the best of both Ridge and Lasso.\n",
        "\n",
        "* **`SGDRegressor`**\n",
        "    * **What it is:** Linear regression trained using **Stochastic Gradient Descent**.\n",
        "    * **How it works:** Updates weights after each sample (or mini-batch).\n",
        "    * **Equivalent to:** Exactly what we discussed in the “Batch vs. Stochastic GD” section.  \n",
        "      Crucial for **very large (Big Data)** datasets that do not fit into memory, because it learns “online,” sample by sample.  \n",
        "      Requires careful data scaling (which we already know how to do!).\n"
      ],
      "metadata": {
        "id": "trm58BlHGxRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Diabetes Dataset\n",
        "\n",
        "The Diabetes dataset is a classic regression dataset included in `scikit-learn`.  \n",
        "It contains **medical diagnostic measurements** of patients and the goal is to predict a **quantitative disease progression score** one year after baseline.\n",
        "\n",
        "**Dataset characteristics:**\n",
        "\n",
        "| Property | Value |\n",
        "|---|---|\n",
        "Number of samples | 442 patients  \n",
        "Number of features | 10 numeric features  \n",
        "Task | Regression — predict disease progression  \n",
        "Target | Disease progression after one year (quantitative score)  \n",
        "\n",
        "**Features description (all continuous numeric variables):**\n",
        "\n",
        "| Feature | Meaning |\n",
        "|---|---|\n",
        "`age` | Age of patient  \n",
        "`sex` | Biological sex  \n",
        "`bmi` | Body mass index  \n",
        "`bp` | Average blood pressure  \n",
        "`S1`–`S6` | Blood serum measurements (lipid & sugar profile)  \n",
        "\n",
        "Note: in this dataset, all input features are **already standardized** to have zero mean and unit variance, but we still apply a `StandardScaler` inside a `Pipeline` for consistency and to demonstrate the correct workflow for general datasets.\n"
      ],
      "metadata": {
        "id": "lYNgkMO8iEr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Application to real-world data (Diabetes)\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Load data\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "print(f\"'Diabetes' dataset loaded:\")\n",
        "print(f\"  Number of features: {X.shape[1]}\")\n",
        "print(f\"  Number of samples:  {X.shape[0]}\")\n",
        "\n",
        "# 2. Train–test split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=RSTATE)\n",
        "\n",
        "# 3. Build a Pipeline\n",
        "# A Pipeline is a \"container\" that executes steps sequentially:\n",
        "# 1) 'scale': Standardizes the data via StandardScaler\n",
        "# 2) 'model': Trains a Ridge regression\n",
        "# This automatically prevents data leakage (the scaler is fit only on X_tr).\n",
        "pipe_ridge = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('model', Ridge(alpha=1.0))  # alpha=1.0 is the default regularization strength\n",
        "])\n",
        "\n",
        "# 4. Training\n",
        "print(\"\\nTraining Pipeline (StandardScaler + Ridge)...\")\n",
        "pipe_ridge.fit(X_tr, y_tr)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# 5. Evaluation on the test set\n",
        "y_pred = pipe_ridge.predict(X_te)\n",
        "\n",
        "rmse_sk = np.sqrt(mean_squared_error(y_te, y_pred))\n",
        "r2_sk = r2_score(y_te, y_pred)\n",
        "\n",
        "print(\"\\n--- Performance on the Diabetes Test Set ---\")\n",
        "print(f\"RMSE: {rmse_sk:.3f}\")\n",
        "print(f\"R²:   {r2_sk:.3f}\")\n",
        "\n",
        "# Show learned coefficients (weights)\n",
        "coefs = pipe_ridge.named_steps['model'].coef_\n",
        "print(f\"\\nLearned coefficients (weights): \\n{coefs.round(2)}\")\n"
      ],
      "metadata": {
        "id": "1lVlyEbRDZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Commentary on Results\n",
        "\n",
        "The model trained on the Diabetes dataset using a standardized **Ridge regression** achieved:\n",
        "\n",
        "- **RMSE ≈ 53**  \n",
        "- **R² ≈ 0.49**\n",
        "\n",
        "This means the model explains **approximately 49% of the variance** in the disease-progression target variable. In other words, it captures about half of the underlying signal — which is **typical performance** for this dataset when using linear models.\n",
        "\n",
        "Linear methods are intentionally simple and interpretable, and may not fully capture complex, nonlinear medical relationships. However, they offer a **strong and transparent baseline**, which is especially valuable in healthcare contexts.\n",
        "\n",
        "### Interpretation of coefficients\n",
        "\n",
        "The learned coefficients show how each feature influences the prediction, assuming the others stay constant:\n",
        "\n",
        "- Positive coefficients (e.g., `bmi`, `S5`) **increase predicted disease progression**\n",
        "- Negative coefficients (e.g., `bp`, `S3`) **decrease predicted progression**\n",
        "\n",
        "Examples:\n",
        "\n",
        "- **BMI (~25.21)** has a strong positive effect — consistent with medical expectations.\n",
        "- **S3 (~-34.26)** has the largest negative effect, indicating an inverse relationship.\n",
        "- **Sex (~-11.44)** shows a moderate influence — note this reflects correlation in this dataset, not causation.\n",
        "\n",
        "Overall, coefficients reveal **multiple competing physiological factors**, which is expected in biomedical data.\n",
        "\n",
        "### Takeaways\n",
        "\n",
        "- Ridge regression provides **reasonable accuracy** and **clear interpretability**\n",
        "- Performance matches known benchmarks for this dataset\n",
        "- For improved performance, future steps could include:\n",
        "  - Polynomial or interaction features\n",
        "  - Tree-based models (Random Forest, Gradient Boosting)\n",
        "  - Non-linear models with regularization\n",
        "  - Neural networks (with tuning)\n",
        "\n",
        "Linear regression remains a valuable **first step and transparent baseline model** in medical ML workflows.\n"
      ],
      "metadata": {
        "id": "R8JFwxK0icqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When Linear Regression Fails (Non-linear Data)\n",
        "\n",
        "Linear regression is a powerful tool, but it has a fundamental limitation:  \n",
        "it **assumes that the relationship between \\(X\\) and \\(y\\) is linear (a straight line)**.\n",
        "\n",
        "If the true relationship is more complex (e.g., sinusoidal, parabolic, exponential), a linear model will fail — no matter how long we train it or how much data we provide.  \n",
        "We’ll demonstrate this using data that follow a sine function.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlOHAtUFHDMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When linear regression fails (non-linear data)\n",
        "\n",
        "# 1. Create strongly non-linear data (sine wave + noise)\n",
        "rng = np.random.default_rng(RSTATE)\n",
        "X_nl = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
        "y_nl = np.sin(X_nl).ravel() + rng.normal(scale=0.3, size=X_nl.shape[0])\n",
        "\n",
        "# 2. Train a simple linear model\n",
        "# No scaler or complex pipeline needed here\n",
        "lin_reg_fail = LinearRegression()\n",
        "lin_reg_fail.fit(X_nl, y_nl)\n",
        "\n",
        "# 3. Predictions\n",
        "y_pred_fail = lin_reg_fail.predict(X_nl)\n",
        "\n",
        "# 4. Evaluation\n",
        "r2_fail = r2_score(y_nl, y_pred_fail)\n",
        "print(f\"--- Performance on non-linear data ---\")\n",
        "print(f\"R²: {r2_fail:.3f}\")\n",
        "print(\"The model failed to explain almost any variance!\")\n",
        "\n",
        "# 5. Visualization of failure\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.scatter(X_nl, y_nl, alpha=0.6, label=\"True non-linear data (sine wave)\")\n",
        "plt.plot(X_nl, y_pred_fail, color='red', lw=3,\n",
        "         label=f\"Linear regression (R² = {r2_fail:.3f})\")\n",
        "plt.title(\"Failure of linear regression on non-linear data\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ph5EfF5XFdoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Polynomial Features\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# 1. Create mildly non-linear data (parabola + noise)\n",
        "rng = np.random.default_rng(RSTATE)\n",
        "X_poly_src = np.linspace(-4, 4, 200).reshape(-1, 1)\n",
        "# y = 0.5*x^2 + 1*x + 2 + noise\n",
        "y_poly = 0.5 * (X_poly_src**2).ravel() + 1.0 * X_poly_src.ravel() + 2.0 + rng.normal(scale=1.5, size=X_poly_src.shape[0])\n",
        "\n",
        "# Train–test split\n",
        "Xtr_p, Xte_p, ytr_p, yte_p = train_test_split(X_poly_src, y_poly, test_size=0.3, random_state=RSTATE)\n",
        "\n",
        "# --- Model 1: Linear baseline ---\n",
        "# This model should fail\n",
        "pipe_lin = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "pipe_lin.fit(Xtr_p, ytr_p)\n",
        "y_pred_lin = pipe_lin.predict(Xte_p)\n",
        "rmse_lin = np.sqrt(mean_squared_error(yte_p, y_pred_lin))\n",
        "\n",
        "# --- Model 2: Polynomial (degree = 2) ---\n",
        "# This model should succeed\n",
        "pipe_poly = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # Step 1: Create x and x^2\n",
        "    ('scale', StandardScaler()),                                 # Step 2: Scale x and x^2\n",
        "    ('model', LinearRegression())                                # Step 3: Fit the model\n",
        "])\n",
        "pipe_poly.fit(Xtr_p, ytr_p)\n",
        "y_pred_poly = pipe_poly.predict(Xte_p)\n",
        "rmse_poly = np.sqrt(mean_squared_error(yte_p, y_pred_poly))\n",
        "\n",
        "# --- Comparison & visualization ---\n",
        "print(f\"RMSE (Linear model):     {rmse_lin:.3f}\")\n",
        "print(f\"RMSE (Polynomial d=2):   {rmse_poly:.3f}  <- Much better!\")\n",
        "\n",
        "# Plot results on the test data\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.scatter(Xte_p, yte_p, alpha=0.6, label=\"Test data (parabola)\")\n",
        "\n",
        "# Sort points for smooth curves\n",
        "sort_idx = np.argsort(Xte_p.ravel())\n",
        "plt.plot(Xte_p[sort_idx], y_pred_lin[sort_idx], color='red', lw=3,\n",
        "         label=f\"Poor linear fit (RMSE: {rmse_lin:.3f})\")\n",
        "plt.plot(Xte_p[sort_idx], y_pred_poly[sort_idx], color='green', lw=3,\n",
        "         label=f\"Good polynomial fit (RMSE: {rmse_poly:.3f})\")\n",
        "\n",
        "plt.title(\"Rescuing Linear Regression with Polynomial Features\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OB7HMp8eF9XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Y4dtTEQFpmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}