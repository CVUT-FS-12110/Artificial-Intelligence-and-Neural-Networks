{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e85852",
   "metadata": {},
   "source": [
    "# Simple Decision Tree Regressor from Scratch\n",
    "\n",
    "In this notebook, we will implement a simple decision tree regressor from scratch. \n",
    "This will give us a deeper understanding of the inner workings of decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ced610",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "We will use NumPy for numerical calculations and Matplotlib for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af05b8",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We will create a synthetic dataset using the `make_regression` function. \n",
    "This dataset will be used to train and visualize the predictions of our decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "plt.plot(X, y, '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149fb0d3",
   "metadata": {},
   "source": [
    "## Implement the Decision Tree Regressor\n",
    "\n",
    "Next, we'll define our decision tree regressor. The tree will be built recursively by choosing the best split based on variance reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple decision tree regressor class.\n",
    "class SimpleDecisionTreeRegressor:\n",
    "    # Initialization method with optional max depth parameter.\n",
    "    # The max depth limits how deep the tree can grow to prevent overfitting.\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree\n",
    "        self.tree = None  # This will hold the built tree\n",
    "\n",
    "    # Method to fit the tree with data.\n",
    "    # X is the feature matrix and y is the target vector.\n",
    "    def fit(self, X, y):\n",
    "        # Build the tree using the training data and starting from depth 0.\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    # Method to build the tree recursively.\n",
    "    # It receives the subset of the data to split, and the current depth of the tree.\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # If the max depth is reached or there are no more unique values in y,\n",
    "        # return the mean value of y as a leaf node.\n",
    "        if depth == self.max_depth or len(set(y)) <= 1:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Find the best split based on the current subset of data.\n",
    "        best_feature, best_threshold, best_score, best_splits = self._best_split(X, y)\n",
    "        # If no split is possible because no feature gives a better score than the variance,\n",
    "        # return the mean value of y as a leaf node.\n",
    "        if best_feature is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        # Recursively build the left subtree using the best split subset.\n",
    "        left_tree = self._build_tree(*best_splits[0], depth + 1)\n",
    "        # Recursively build the right subtree using the best split subset.\n",
    "        right_tree = self._build_tree(*best_splits[1], depth + 1)\n",
    "        # Return a dictionary representing the current node split with the feature index,\n",
    "        # the threshold value, and the left/right subtrees.\n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left_tree, 'right': right_tree}\n",
    "\n",
    "    # Method to find the best split for a node.\n",
    "    # It iterates over all features and their possible threshold values to find the split\n",
    "    # that results in the largest reduction in variance (best score).\n",
    "    def _best_split(self, X, y):\n",
    "        m, n = X.shape  # m = number of samples, n = number of features\n",
    "        # Start with the worst score; the total variance of the target times the number of samples.\n",
    "        best_score = np.var(y) * len(y)\n",
    "        # Initialize variables to store the best feature index, threshold value, and splits.\n",
    "        best_feature, best_threshold, best_splits = None, None, None\n",
    "\n",
    "        # Iterate over all features.\n",
    "        for feature in range(n):\n",
    "            # Get all unique values of the feature as potential threshold values.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Iterate over all thresholds.\n",
    "            for threshold in thresholds:\n",
    "                # Split the data based on the current feature and threshold.\n",
    "                splits = self._split(X, y, feature, threshold)\n",
    "                # Calculate the score as the weighted average of the variance of the two splits.\n",
    "                score = sum(np.var(split[1]) * len(split[1]) for split in splits)\n",
    "\n",
    "                # If the calculated score is better (lower) than the current best score,\n",
    "                # update the best split variables with the current feature, threshold, and splits.\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_splits = splits\n",
    "\n",
    "        # Return the best feature index, threshold, score, and splits.\n",
    "        return best_feature, best_threshold, best_score, best_splits\n",
    "\n",
    "    # Method to split the data based on a given feature and threshold.\n",
    "    # It returns the subsets of data for the left and right nodes after the split.\n",
    "    def _split(self, X, y, feature, threshold):\n",
    "        # Create a boolean mask for the rows that go to the left child (where feature value is less than threshold).\n",
    "        left_mask = X[:, feature] < threshold\n",
    "        # Create a boolean mask for the rows that go to the right child (where feature value is not less than threshold).\n",
    "        right_mask = ~left_mask\n",
    "        # Split the data into left and right based on the masks and return the subsets.\n",
    "        return (X[left_mask], y[left_mask]), (X[right_mask], y[right_mask])\n",
    "\n",
    "    # Method to predict the target for each sample in the feature matrix X.\n",
    "    # It applies the learned tree rules to each sample to make predictions.\n",
    "    def predict(self, X):\n",
    "        # Use a list comprehension to apply the prediction to each sample in X.\n",
    "        # For each record in X, we call the _predict_one method, which traverses the tree.\n",
    "        return np.array([self._predict_one(record, self.tree) for record in X])\n",
    "    \n",
    "    # Helper method to predict a single data point by traversing the tree.\n",
    "    def _predict_one(self, x, tree):\n",
    "        # If the current node is not a dictionary, it means we have reached a leaf node.\n",
    "        # Leaf nodes do not have a 'feature' or 'threshold', but just a single value (the mean of the target values).\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree  # Return the value of the leaf node\n",
    "        \n",
    "        # Check the feature value of the sample against the threshold of the current node.\n",
    "        # Depending on whether it's lower or higher, move to the left or right child node, respectively.\n",
    "        if x[tree['feature']] < tree['threshold']:\n",
    "            return self._predict_one(x, tree['left'])  # Traverse left subtree\n",
    "        else:\n",
    "            return self._predict_one(x, tree['right'])  # Traverse right subtree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c1e79",
   "metadata": {},
   "source": [
    "## Train the Model and Make Predictions\n",
    "\n",
    "With our decision tree defined, we can now train it on our dataset and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tree and fit to the data\n",
    "tree = SimpleDecisionTreeRegressor(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Make predictions over the data\n",
    "predictions = tree.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a61afc",
   "metadata": {},
   "source": [
    "## Plotting the Results\n",
    "\n",
    "Finally, we will plot the actual data points against the predictions made by our decision tree to visually assess its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99437639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual data points\n",
    "plt.scatter(X, y, color='blue', label='Actual')\n",
    "# Plot the predicted data points\n",
    "plt.scatter(X, predictions, color='red', label='Predicted')\n",
    "# Set plot title and labels\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "# Show legend\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef04d6-cea3-44b4-ab8d-5bb2e4e58916",
   "metadata": {},
   "source": [
    "### Comaparison with Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03ced1-ae85-4bcd-aeef-c1345d76e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and train the simple decision tree regressor\n",
    "simple_tree = SimpleDecisionTreeRegressor(max_depth=3)\n",
    "simple_tree.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate and train the random forest regressor from scikit-learn\n",
    "rf_regressor = RandomForestRegressor(max_depth=3, n_estimators=10, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with both models on the test set\n",
    "simple_tree_predictions = simple_tree.predict(X_test)\n",
    "rf_predictions = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of both models\n",
    "simple_tree_mse = mean_squared_error(y_test, simple_tree_predictions)\n",
    "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
    "\n",
    "# Print the performance\n",
    "print(f\"Simple Decision Tree MSE: {simple_tree_mse}\")\n",
    "print(f\"Random Forest MSE: {rf_mse}\")\n",
    "\n",
    "# Visual comparison of actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot for simple decision tree\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, simple_tree_predictions, color='red', label='Predicted')\n",
    "plt.title('Simple Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for random forest\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, rf_predictions, color='green', label='Predicted')\n",
    "plt.title('Random Forest Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
