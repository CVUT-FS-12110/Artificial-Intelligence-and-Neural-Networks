{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FemXcqR0dBUp",
        "QC1zQ273Ehxi",
        "RRkFym6rF96s",
        "xJ6AFQ20GCjF",
        "p_Q_M06LGVSA",
        "npK-gs7JMAQH",
        "yJw8GAechVIl",
        "Db3LfuWIlgJd",
        "R7dVe2X9nJ15",
        "o_EONYRtudud",
        "f9XfzcQ9uibF",
        "V2h9AU3TyVTO",
        "7ynU8HGnJLUI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Imports\n",
        "\n",
        "Install and import everything we need."
      ],
      "metadata": {
        "id": "vsigwlHYc6pP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcK1-NpYa5fa"
      },
      "outputs": [],
      "source": [
        "!pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "nuauW_qRdXhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal Working Example - Profiling\n",
        "\n",
        "Check whether the library is working."
      ],
      "metadata": {
        "id": "FemXcqR0dBUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(np.random.rand(100, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()\n",
        "profile.to_file(\"report.html\")\n"
      ],
      "metadata": {
        "id": "3x7qrQJ2dOgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Import\n",
        "\n",
        "Examples of various dataset imports."
      ],
      "metadata": {
        "id": "QC1zQ273Ehxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn Datasets"
      ],
      "metadata": {
        "id": "RRkFym6rF96s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8sb6aLdJEpIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seaborn Dataset"
      ],
      "metadata": {
        "id": "xJ6AFQ20GCjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.get_dataset_names()\n"
      ],
      "metadata": {
        "id": "3UikR5ZOE_qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset(\"flights\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "joCzj2GrFSzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statsmodels Datasets"
      ],
      "metadata": {
        "id": "p_Q_M06LGVSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "df = sm.datasets.get_rdataset(\"AirPassengers\").data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ngjixNi5GLDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Example Profiling - Titanic\n"
      ],
      "metadata": {
        "id": "z4lar9QaKZ1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "ProfileReport(df, title=\"Report\", explorative=True).to_notebook_iframe()"
      ],
      "metadata": {
        "id": "CHv8osBMKcGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Example Profiling - Adult Income"
      ],
      "metadata": {
        "id": "npK-gs7JMAQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "df = pd.read_csv(url, header=None, na_values=\"?\", skipinitialspace=True)\n",
        "ProfileReport(df, title=\"Report\", explorative=True).to_notebook_iframe()"
      ],
      "metadata": {
        "id": "Zd8yJZyyMFej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jou35nCYMHmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality Improvement Example - Dummy Customer Data\n",
        "\n"
      ],
      "metadata": {
        "id": "yJw8GAechVIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Dataset Generartion"
      ],
      "metadata": {
        "id": "Db3LfuWIlgJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"ID\": range(1, 21),\n",
        "    \"Age\": [25, 102, np.nan, 40, 28, 15, 300, 22, 33, np.nan, 27, 35, 41, 22, 22, 25, 26, 25, 120, 17],\n",
        "    \"Gender\": [\"M\", \"M\", \"F\", None, \"M\", \"Male\", \"femlae\", \"M\", \"F\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", None, \"M\", \"M\", \"F\", \"F\"],\n",
        "    \"Income\": [45000, -10000, 21050, 67000, 43100, 16000, 110000, np.nan, 27000, 26000, 28000, 29000, None, 22000, 31000, 1001000, 32500, 33000, 34800, 35000],\n",
        "    \"Purchases\": [5, 3, np.nan, 0, 100, 2, 1, 3, 2, 3, 1, 3, 3, 5, 3, 4, 3, 5, 3, 3],\n",
        "    \"Country\": [\"CZ\", \"DE\", \"FR\", \"UK\", None, \"CZ\", \"CZ\", \"UK\", \"CZ\", \"FR\", \"CZ\", \"CZ\", \"DE\", \"CZ\", \"CZ\", \"CZ\", \"CZ\", \"CZ\", \"CZ\", \"CZ\"],\n",
        "    \"Rating\": [4, 5, 6, 3, np.nan, 2, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"uncleaned_dataset.csv\", index=False)\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "NRdLNHdwhbUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Profilling"
      ],
      "metadata": {
        "id": "R7dVe2X9nJ15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "JHmxr80VkFlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing the Issues"
      ],
      "metadata": {
        "id": "8lF2KVvvmAYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index(\"ID\", drop=True, inplace=True)\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "owC4cTdbmDqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_age, max_age = 18, 110\n",
        "df.loc[(df[\"Age\"] < min_age) | (df[\"Age\"] > max_age), \"Age\"] = np.nan\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "ontRlAQnntAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_purchases = 10\n",
        "df.loc[(df[\"Purchases\"] > max_purchases), \"Purchases\"] = np.nan\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "Y8sN-92wokdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_income, max_income = 0, 500000\n",
        "df.loc[(df[\"Income\"] < min_income) | (df[\"Income\"] > max_income), \"Income\"] = np.nan\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "5leGQtSZmpFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Rating\"] = df[\"Rating\"].fillna(df[\"Rating\"].median())\n",
        "df.head(21)"
      ],
      "metadata": {
        "id": "XAvYP4dCqchQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df.dropna()\n",
        "clean_df"
      ],
      "metadata": {
        "id": "yiQm-ukdqyDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(clean_df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "1aqz5BvhrKz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality Improvement Example - Time Series"
      ],
      "metadata": {
        "id": "R6lcmB3wseG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Creation and Profilling"
      ],
      "metadata": {
        "id": "o_EONYRtudud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series_dataset(n=200, freq=\"1h\", seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # --- Time index ---\n",
        "    time_index = pd.date_range(\"2025-01-01\", periods=n, freq=freq)\n",
        "\n",
        "    # --- Base signal (trend + noise) ---\n",
        "    base = np.linspace(0, 10, n) + np.random.normal(0, 0.5, n)\n",
        "\n",
        "    # --- Derived signals ---\n",
        "    signal_A = np.sin(base) + np.random.normal(3, 0.1, n)\n",
        "    signal_B = np.cos(base) + np.random.normal(-2, 0.5, n)\n",
        "    signal_C = 0.5 * signal_A + 0.3 * np.random.normal(1, 3, n)  # correlated with A\n",
        "    signal_D = np.random.normal(-100, 10, n)                         # uncorrelated noise\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"time\": time_index,\n",
        "        \"signal_A\": signal_A,\n",
        "        \"signal_B\": signal_B,\n",
        "        \"signal_C\": signal_C,\n",
        "        \"signal_D\": signal_D,\n",
        "    }).set_index(\"time\")\n",
        "\n",
        "    # --- Randomly drop some rows (simulate missing timestamps) ---\n",
        "    drop_idx = np.random.choice(df.index, size=int(0.1 * n), replace=False)\n",
        "    df = df.drop(drop_idx)\n",
        "\n",
        "    # --- Inject NaNs into random cells ---\n",
        "    for col in df.columns:\n",
        "        nan_idx = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
        "        df.loc[nan_idx, col] = np.nan\n",
        "\n",
        "    # --- Add duplicate rows ---\n",
        "    dup_idx = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
        "    duplicates = df.loc[dup_idx]\n",
        "    df = pd.concat([df, duplicates]).sort_index()\n",
        "\n",
        "    return df\n",
        "\n",
        "df = generate_time_series_dataset()\n",
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "7G7f9DBXsm8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing the Issues"
      ],
      "metadata": {
        "id": "f9XfzcQ9uibF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Duplicate Rows - Removal"
      ],
      "metadata": {
        "id": "8nwS-kZjvfCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df.index.duplicated(keep=\"first\")]"
      ],
      "metadata": {
        "id": "2dvB7wmNvidO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Rows - Reindexing"
      ],
      "metadata": {
        "id": "hd5YHIu2u6d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_idx = pd.date_range(df.index.min(), df.index.max(), freq=\"1h\")\n",
        "df = df.reindex(full_idx)   # missing timestamps will be NaN\n",
        "df.index.name = \"time\""
      ],
      "metadata": {
        "id": "4BU8eVZRuoq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Cells - Interpolation"
      ],
      "metadata": {
        "id": "GvQPlzFyyKpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.interpolate(method=\"time\", limit_direction=\"both\")"
      ],
      "metadata": {
        "id": "UZX0Y401yNlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization (If Required)"
      ],
      "metadata": {
        "id": "R9gT3BU93L4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (df - df.mean()) / df.std()"
      ],
      "metadata": {
        "id": "q2uF_7jV3Kwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization and Scaling (More Examples)"
      ],
      "metadata": {
        "id": "C6ROtfGzucn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = 50 + 20 * np.sin(x) + np.random.normal(0, 5, size=len(x))  # offset + large std\n",
        "\n",
        "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[\"y_zscore\"] = scaler.fit_transform(df[[\"y\"]])\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Line plots\n",
        "axes[0, 0].plot(df[\"x\"], df[\"y\"], color=\"orange\")\n",
        "axes[0, 0].set_title(\"Original Data (Offset + High Std)\")\n",
        "axes[0, 0].set_xlabel(\"x\")\n",
        "axes[0, 0].set_ylabel(\"y\")\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "axes[0, 1].plot(df[\"x\"], df[\"y_zscore\"], color=\"blue\")\n",
        "axes[0, 1].set_title(\"After Z-Score Normalization\")\n",
        "axes[0, 1].set_xlabel(\"x\")\n",
        "axes[0, 1].set_ylabel(\"z-score\")\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# Histograms\n",
        "axes[1, 0].hist(df[\"y\"], bins=20, color=\"orange\", edgecolor=\"black\")\n",
        "axes[1, 0].set_title(\"Original Data Distribution\")\n",
        "axes[1, 0].set_xlabel(\"Value\")\n",
        "axes[1, 0].set_ylabel(\"Frequency\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(df[\"y_zscore\"], bins=20, color=\"blue\", edgecolor=\"black\")\n",
        "axes[1, 1].set_title(\"Z-Scored Data Distribution\")\n",
        "axes[1, 1].set_xlabel(\"Z-Score\")\n",
        "axes[1, 1].set_ylabel(\"Frequency\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HNjD5BMluZaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.random.uniform(0, 255, size=len(x))\n",
        "\n",
        "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[\"y_scaled\"] = scaler.fit_transform(df[[\"y\"]])\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "axes[0, 0].plot(df[\"x\"], df[\"y\"], color=\"orange\")\n",
        "axes[0, 0].set_title(\"Original Data (Uniform 0â€“255)\")\n",
        "axes[0, 0].set_xlabel(\"x\")\n",
        "axes[0, 0].set_ylabel(\"Value\")\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "axes[0, 1].plot(df[\"x\"], df[\"y_scaled\"], color=\"blue\")\n",
        "axes[0, 1].set_title(\"After Minâ€“Max Scaling (0â€“1)\")\n",
        "axes[0, 1].set_xlabel(\"x\")\n",
        "axes[0, 1].set_ylabel(\"Scaled Value\")\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "axes[1, 0].hist(df[\"y\"], bins=20, color=\"orange\", edgecolor=\"black\")\n",
        "axes[1, 0].set_title(\"Original Data Distribution\")\n",
        "axes[1, 0].set_xlabel(\"Value\")\n",
        "axes[1, 0].set_ylabel(\"Frequency\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(df[\"y_scaled\"], bins=20, color=\"blue\", edgecolor=\"black\")\n",
        "axes[1, 1].set_title(\"Minâ€“Max Scaled Distribution\")\n",
        "axes[1, 1].set_xlabel(\"Scaled Value\")\n",
        "axes[1, 1].set_ylabel(\"Frequency\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UrKjrB0fwbFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "CD0UgKl1Ii7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# --- Original dataset ---\n",
        "df = pd.DataFrame({\n",
        "    \"gene\": [\n",
        "        \"GAA_01\", \"GAA_02\", \"GAB_01\", \"GAB_02\", \"GAC_05\",\n",
        "        \"GAA_03\", \"GAC_01\", \"GAB_03\", \"GAD_01\", \"GAA_10\"\n",
        "    ],\n",
        "    \"color\": [\"red\", \"green\", \"blue\", \"red\", \"green\", \"blue\", \"red\", \"green\", \"blue\", \"red\"],\n",
        "    \"satisfaction\": [\"low\", \"medium\", \"high\", \"medium\", \"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\"],\n",
        "    \"country\": [\"CZ\", \"DE\", \"CZ\", \"DE\", \"CZ\", \"DE\", \"CZ\", \"DE\", \"CZ\", \"DE\"],\n",
        "    \"product_type\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\", \"C\", \"A\"],\n",
        "    \"sales\": [200, 310, 250, 420, 300, 215, 480, 330, 410, 260]\n",
        "})\n",
        "\n",
        "print(\"ðŸ§¬ Original DataFrame:\\n\")\n",
        "print(df, \"\\n\")\n",
        "\n",
        "# --- Semi-ordinal encoding for gene ---\n",
        "families = sorted({g.split(\"_\")[0][1:] for g in df[\"gene\"]})  # e.g. [\"AA\", \"AB\", \"AC\", \"AD\"]\n",
        "family_to_index = {fam: i for i, fam in enumerate(families)}\n",
        "\n",
        "def gene_semi_ordinal_score(gene_code: str) -> int:\n",
        "    family, variant = gene_code.split(\"_\")\n",
        "    fam_key = family[1:] if family.startswith(\"G\") else family\n",
        "    fam_index = family_to_index.get(fam_key, -1)\n",
        "    return fam_index * 100 + int(variant)\n",
        "\n",
        "df[\"gene_enc\"] = df[\"gene\"].map(gene_semi_ordinal_score)\n",
        "\n",
        "# --- Binary encoding for country ---\n",
        "df[\"country_enc\"] = df[\"country\"].map({\"CZ\": 0, \"DE\": 1})\n",
        "\n",
        "# --- One-hot encoding for color ---\n",
        "df_color = pd.get_dummies(df[\"color\"], prefix=\"color\")\n",
        "\n",
        "# --- Ordinal encoding for satisfaction ---\n",
        "ord_enc = OrdinalEncoder(categories=[[\"low\", \"medium\", \"high\"]])\n",
        "df[\"satisfaction_enc\"] = ord_enc.fit_transform(df[[\"satisfaction\"]]).astype(int)\n",
        "\n",
        "# --- Target / mean encoding for product_type ---\n",
        "product_mean = df.groupby(\"product_type\")[\"sales\"].mean().to_dict()\n",
        "df[\"product_type_enc\"] = df[\"product_type\"].map(product_mean)\n",
        "\n",
        "# --- Combine encoded dataset ---\n",
        "df_encoded = pd.concat([\n",
        "    df[[\"gene_enc\", \"country_enc\", \"satisfaction_enc\", \"product_type_enc\", \"sales\"]],\n",
        "    df_color\n",
        "], axis=1)\n",
        "\n",
        "print(\"ðŸ”¢ Encoded DataFrame:\\n\")\n",
        "print(df_encoded)"
      ],
      "metadata": {
        "id": "_PkqbgN8GQpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Profilling"
      ],
      "metadata": {
        "id": "V2h9AU3TyVTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "8FbzDa-SyPSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction / Dimmension Reduction\n",
        "\n",
        "Examples with PCA and LDA"
      ],
      "metadata": {
        "id": "1lNodGLeJEyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis"
      ],
      "metadata": {
        "id": "7ynU8HGnJLUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time index\n",
        "t = np.linspace(0, 10, 500)  # 500 samples from 0 to 10 seconds\n",
        "\n",
        "# Create 5 unique base signals (sinusoids with different frequencies and phases)\n",
        "base_signals = {\n",
        "    f\"signal_{i+1}\": np.sin(t * (i+1) + i)\n",
        "    for i in range(5)\n",
        "}\n",
        "\n",
        "# Create correlated variants (each base signal gets 4 noisy copies)\n",
        "data = {}\n",
        "for name, base in base_signals.items():\n",
        "    for j in range(4):\n",
        "        data[f\"{name}_var{j+1}\"] = base + np.random.normal(0, 0.05, len(base))  # small Gaussian noise\n",
        "\n",
        "# Combine into DataFrame\n",
        "df = pd.DataFrame(data, index=pd.Index(t, name=\"time\"))\n",
        "\n",
        "# Add some artificial imperfections\n",
        "df = df.sort_index()  # and sort back by time\n",
        "\n",
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()\n"
      ],
      "metadata": {
        "id": "uS9GBCwWJSuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "n = 5\n",
        "pca = PCA(n_components=n)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "df_pca = pd.DataFrame(X_pca, index=df.index, columns=[f\"PC{i+1}\" for i in range(n)])\n",
        "\n",
        "pca_full = PCA(n_components=df.shape[1])\n",
        "pca_full.fit(X_scaled)\n",
        "\n",
        "explained = pca_full.explained_variance_ratio_\n",
        "cumulative = np.cumsum(explained)\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(8, 9), sharex=False)\n",
        "\n",
        "# Original signals\n",
        "axes[0].plot(df.index, df, alpha=0.5)\n",
        "axes[0].set_title(\"Original 20 Correlated Signals\")\n",
        "axes[0].set_ylabel(\"Amplitude\")\n",
        "\n",
        "# Top n PCA components\n",
        "axes[1].plot(df_pca.index, df_pca)\n",
        "axes[1].set_title(f\"Top {n} Principal Components (PCA)\")\n",
        "axes[1].set_xlabel(\"Time\")\n",
        "axes[1].set_ylabel(\"Component Value\")\n",
        "\n",
        "# Explained variance\n",
        "axes[2].bar(range(1, len(explained)+1), explained, alpha=0.7, label=\"Individual variance\")\n",
        "axes[2].plot(range(1, len(cumulative)+1), cumulative, color='r', marker='o', label=\"Cumulative variance\")\n",
        "axes[2].set_title(\"Explained Variance by Principal Components\")\n",
        "axes[2].set_xlabel(\"Principal Component\")\n",
        "axes[2].set_ylabel(\"Explained Variance Ratio\")\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zC3ZEn4GKopp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Discriminant Analysis"
      ],
      "metadata": {
        "id": "i05ck7ZvtFzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Synthetic signal dataset ---\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "t = np.linspace(0, 2*np.pi, 10)  # 10 time points per sample\n",
        "\n",
        "def make_class(base_amp, noise, phase_shift):\n",
        "    \"\"\"Generate noisy sinusoidal samples for one class.\"\"\"\n",
        "    return base_amp * np.sin(t + phase_shift) + np.random.normal(0, noise, (n_samples, len(t)))\n",
        "\n",
        "# More overlapping amplitudes and random phase shifts\n",
        "normal = make_class(1.0, 0.15, 0.0)\n",
        "overheated = make_class(1.2, 0.15, 0.2)\n",
        "underloaded = make_class(0.8, 0.15, -0.3)\n",
        "\n",
        "# Combine\n",
        "X = np.vstack([normal, overheated, underloaded])\n",
        "y = np.array([\"Normal\"]*n_samples + [\"Overheated\"]*n_samples + [\"Underloaded\"]*n_samples)\n",
        "\n",
        "# Put into DataFrame\n",
        "df = pd.DataFrame(X, columns=[f\"t{i+1}\" for i in range(X.shape[1])])\n",
        "df[\"state\"] = y\n",
        "\n",
        "profile = ProfileReport(df, title=\"Report\", explorative=True)\n",
        "profile.to_notebook_iframe()\n",
        "\n"
      ],
      "metadata": {
        "id": "3ujubtJ6tM4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=\"state\").values\n",
        "y = df[\"state\"].values\n",
        "\n",
        "# Fit LDA (reduce to 2D for visualization)\n",
        "lda = LDA(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# Plot LDA projection\n",
        "plt.figure(figsize=(8, 5))\n",
        "for label, color in zip([\"Normal\", \"Overheated\", \"Underloaded\"], [\"blue\", \"red\", \"green\"]):\n",
        "    plt.scatter(X_lda[y == label, 0], X_lda[y == label, 1], label=label, alpha=0.7)\n",
        "plt.xlabel(\"LDA Component 1\")\n",
        "plt.ylabel(\"LDA Component 2\")\n",
        "plt.title(\"Linear Discriminant Analysis of Machine States\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wOj1KyXr1ndM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Stratification"
      ],
      "metadata": {
        "id": "K0gtNw8mifdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Stratification"
      ],
      "metadata": {
        "id": "LxWCn6SJRLzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset\n",
        "df = pd.DataFrame({\n",
        "    \"feature1\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    \"feature2\": [10, 20, 30, 40, 50, 60, 70, 80],\n",
        "    \"label\":    [\"A\", \"A\", \"B\", \"B\", \"A\", \"A\", \"B\", \"B\"]\n",
        "})\n",
        "\n",
        "# Split DataFrame while keeping class ratios the same\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.25,\n",
        "    stratify=df[\"label\"],   # stratify by label column\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training\")\n",
        "print(train_df.head())\n",
        "print(\"Validation\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "id": "JVLCKGUuimlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binning Stratification"
      ],
      "metadata": {
        "id": "RXjf9NVwRRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"age\": [12, 18, 25, 33, 37, 42, 47, 52, 56, 61, 65, 70, 75, 19, 22, 28, 39, 44, 58, 63, 18],\n",
        "    \"income\": [25000, 27000, 32000, 35000, 37000, 40000, 42000, 45000, 48000, 39000,\n",
        "               50000, 52000, 54000, 55000, 29000, 31000, 33000, 38000, 41000, 47000, 51000]\n",
        "})\n",
        "\n",
        "df[\"age_bin\"] = pd.cut(df[\"age\"], bins=[0, 20, 40, 60, 80], labels=[\"<20\", \"20â€“40\", \"40â€“60\", \"60â€“80\"])\n",
        "\n",
        "train, test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.3,\n",
        "    stratify=df[\"age_bin\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\")\n",
        "print(train[[\"age\", \"age_bin\"]].sort_values(\"age\"))\n",
        "\n",
        "print(\"\\nTest set:\")\n",
        "print(test[[\"age\", \"age_bin\"]].sort_values(\"age\"))\n",
        "\n",
        "print(\"\\nAge bin distribution:\")\n",
        "print(\"Train:\")\n",
        "print(train[\"age_bin\"].value_counts(normalize=True))\n",
        "print(\"Test:\")\n",
        "print(test[\"age_bin\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "Td17gEtfMpRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compound Stratification"
      ],
      "metadata": {
        "id": "4V4kI8nuRVvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"age\": np.random.randint(18, 75, size=500),\n",
        "    \"income\": np.random.normal(40000, 12000, size=500).astype(int),\n",
        "    \"employment\": np.random.choice([\"employed\", \"unemployed\"], size=500, p=[0.8, 0.2])\n",
        "})\n",
        "\n",
        "df[\"age_group\"] = pd.cut(\n",
        "    df[\"age\"], bins=[0, 20, 40, 60, 80], labels=[\"<20\", \"20â€“40\", \"40â€“60\", \"60â€“80\"]\n",
        ")\n",
        "\n",
        "df[\"strata\"] = df[\"employment\"].astype(str) + \"_\" + df[\"age_group\"].astype(str)\n",
        "\n",
        "train, test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.3,\n",
        "    stratify=df[\"strata\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\")\n",
        "print(train[[\"age\", \"age_group\", \"employment\", \"strata\"]].sort_values(\"age\"))\n",
        "\n",
        "print(\"\\nTest set:\")\n",
        "print(test[[\"age\", \"age_group\", \"employment\", \"strata\"]].sort_values(\"age\"))\n",
        "\n",
        "print(\"\\nStrata distribution (train vs test):\")\n",
        "print(train[\"strata\"].value_counts(normalize=True).sort_index())\n",
        "print(test[\"strata\"].value_counts(normalize=True).sort_index())\n"
      ],
      "metadata": {
        "id": "p_9pWgH7Qmnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Pipelines"
      ],
      "metadata": {
        "id": "wpgII4G8EcJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1ï¸âƒ£ Generate random data with missing values ---\n",
        "np.random.seed(42)\n",
        "n = 30\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"age\": np.random.randint(18, 60, size=n).astype(float),\n",
        "    \"income\": np.random.normal(50000, 8000, size=n),\n",
        "    \"gender\": np.random.choice([\"M\", \"F\", None], size=n, p=[0.45, 0.45, 0.10]),\n",
        "    \"city\": np.random.choice([\"Prague\", \"Berlin\", \"Paris\", None], size=n, p=[0.3, 0.3, 0.3, 0.1])\n",
        "})\n",
        "\n",
        "# Introduce additional missing values\n",
        "df.loc[np.random.choice(df.index, 5, replace=False), \"age\"] = np.nan\n",
        "df.loc[np.random.choice(df.index, 5, replace=False), \"income\"] = np.nan\n",
        "\n",
        "print(\"ðŸ§¾ Original DataFrame (first 10 rows):\\n\")\n",
        "print(df.head(10), \"\\n\")\n",
        "\n",
        "# --- 2ï¸âƒ£ Split into train and test sets manually ---\n",
        "df_train = df.iloc[:20].copy()\n",
        "df_test = df.iloc[20:].copy()\n",
        "\n",
        "# --- 3ï¸âƒ£ Define preprocessing pipeline ---\n",
        "numeric_features = [\"age\", \"income\"]\n",
        "categorical_features = [\"gender\", \"city\"]\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_transformer, numeric_features),\n",
        "    (\"cat\", categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# --- 4ï¸âƒ£ Fit the pipeline on train data and transform both ---\n",
        "X_train_processed = preprocessor.fit_transform(df_train)\n",
        "X_test_processed = preprocessor.transform(df_test)\n",
        "\n",
        "# --- 5ï¸âƒ£ Convert results back to DataFrames for clarity ---\n",
        "train_cols = (\n",
        "    numeric_features +\n",
        "    list(preprocessor.named_transformers_[\"cat\"].named_steps[\"encoder\"].get_feature_names_out(categorical_features))\n",
        ")\n",
        "df_train_processed = pd.DataFrame(X_train_processed, columns=train_cols)\n",
        "df_test_processed = pd.DataFrame(X_test_processed, columns=train_cols)\n",
        "\n",
        "print(\"âœ… Processed Training Data (first 5 rows):\\n\")\n",
        "print(df_train_processed.head(), \"\\n\")\n",
        "\n",
        "print(\"ðŸ§© Processed Test Data (first 5 rows):\\n\")\n",
        "print(df_test_processed.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "1M_NoauZEfCq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}